{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification of the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading and preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_val' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 59\u001b[0m\n\u001b[1;32m     56\u001b[0m     df_test[column] \u001b[38;5;241m=\u001b[39m df_test[column]\u001b[38;5;241m.\u001b[39mround(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     58\u001b[0m \u001b[38;5;66;03m# add week and day sin cos features\u001b[39;00m\n\u001b[0;32m---> 59\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m df_tmp \u001b[38;5;129;01min\u001b[39;00m [df_train, \u001b[43mdf_val\u001b[49m, df_test]:\n\u001b[1;32m     60\u001b[0m     df_tmp[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mday_sin\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m df_tmp[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mday_of_week\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: np\u001b[38;5;241m.\u001b[39msin((x\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m*\u001b[39m(\u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39mnp\u001b[38;5;241m.\u001b[39mpi\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m6\u001b[39m)))\n\u001b[1;32m     61\u001b[0m     df_tmp[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mday_cos\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m df_tmp[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mday_of_week\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: np\u001b[38;5;241m.\u001b[39mcos((x\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m*\u001b[39m(\u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39mnp\u001b[38;5;241m.\u001b[39mpi\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m6\u001b[39m)))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_val' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import datetime\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "n_classes = 10\n",
    "# data\n",
    "X_features = [\n",
    "    \"mood\",\"circumplex.arousal\",\"circumplex.valence\",\"activity\",\"screen\",\n",
    "    \"call\",\"sms\", \"day_sin\", \"day_cos\", \"day_year_sin\", \"day_year_cos\", \"appCat_total\", \"appCat.builtin\", \"appCat.communication\",\n",
    "    \"appCat.entertainment\", \"appCat.finance\", \"appCat.game\", \"appCat.office\",\n",
    "    \"appCat.social\", \"appCat.travel\", \"appCat.unknown\", \"appCat.utilities\", \"appCat.weather\"\n",
    "]\n",
    "Y_features = [\n",
    "    \"mood_of_next_day\"\n",
    "]\n",
    "df_data = pd.read_csv(\"./data/imputed_data.csv\")\n",
    "df_data[\"datetime\"] = pd.to_datetime(df_data[\"datetime\"])\n",
    "\n",
    "# Reduce mood score to 5 classes\n",
    "#df_data[\"mood\"] = df_data[\"mood\"] / 2\n",
    "#df_data[Y_features[0]] = df_data[Y_features[0]] / 2\n",
    "\n",
    "\n",
    "df_test = pd.DataFrame()\n",
    "df_train = pd.DataFrame()\n",
    "for subject_id in df_data[\"id\"].unique():\n",
    "    df_subject = df_data[df_data[\"id\"]==subject_id]\n",
    "\n",
    "    df_subject = df_subject.sort_values(by=\"datetime\")\n",
    "    split_index = int(0.8 * len(df_subject))\n",
    "    #subject_test_indices = TimeSeriesSplit(n_splits=2, test_size=len(df_subject)*0.2).split(df_subject)\n",
    "    df_train = pd.concat([df_train, df_subject.iloc[:split_index]])\n",
    "    df_test = pd.concat([df_test, df_subject.iloc[split_index:]])\n",
    "\n",
    "#df_train = pd.read_csv(\"./data/train_data_imputed.csv\", index_col=0)\n",
    "#df_train[\"time\"] = pd.to_datetime(df_train[\"time\"])\n",
    "#df_test = pd.read_csv(\"./data/test_data_imputed.csv\", index_col=0)\n",
    "#df_test[\"time\"] = pd.to_datetime(df_test[\"time\"])\n",
    "\n",
    "\n",
    "for column in [\"screen\", \"call\",\"sms\",*X_features[11:]]:\n",
    "    df_train[column] = (df_train[column] - df_train[column].mean())/df_train[column].std()\n",
    "    df_test[column] = (df_test[column] - df_test[column].mean())/df_test[column].std()\n",
    "\n",
    "min_c, max_c = -2, 2\n",
    "for column in [\"circumplex.arousal\", \"circumplex.valence\"]:\n",
    "    df_train[column] = (df_train[column] - min_c) / (max_c - min_c)\n",
    "    df_test[column] = (df_test[column] - min_c) / (max_c - min_c)\n",
    "\n",
    "for column in [\"mood\", \"mood_of_next_day\"]:\n",
    "    df_train[column] = df_train[column].round(0)\n",
    "    df_test[column] = df_test[column].round(0)\n",
    "\n",
    "# add week and day sin cos features\n",
    "for df_tmp in [df_train, df_val, df_test]:\n",
    "    df_tmp[\"day_sin\"] = df_tmp[\"day_of_week\"].apply(lambda x: np.sin((x-1)*(2*np.pi/6)))\n",
    "    df_tmp[\"day_cos\"] = df_tmp[\"day_of_week\"].apply(lambda x: np.cos((x-1)*(2*np.pi/6)))\n",
    "    df_tmp[\"day_year_sin\"] = df_tmp[\"datetime\"].dt.isocalendar().week.apply(lambda x: np.sin((x-1)*(2*np.pi/52)))\n",
    "    df_tmp[\"day_year_cos\"] = df_tmp[\"datetime\"].dt.isocalendar().week.apply(lambda x: np.cos((x-1)*(2*np.pi/52)))\n",
    "\n",
    "#df_test[\"day_sin\"] = df_test[\"day_of_week\"].apply(lambda x: np.sin((x-1)*(2*np.pi/6)))\n",
    "#df_test[\"day_cos\"] = df_test[\"day_of_week\"].apply(lambda x: np.cos((x-1)*(2*np.pi/6)))\n",
    "\n",
    "#df_train[\"day_year_sin\"] = df_train[\"datetime\"].dt.isocalendar().week.apply(lambda x: np.sin((x-1)*(2*np.pi/52)))\n",
    "#df_train[\"day_year_cos\"] = df_train[\"datetime\"].dt.isocalendar().week.apply(lambda x: np.cos((x-1)*(2*np.pi/52)))\n",
    "#df_test[\"day_year_sin\"] = df_test[\"datetime\"].dt.isocalendar().week.apply(lambda x: np.sin((x-1)*(2*np.pi/52)))\n",
    "#df_test[\"day_year_cos\"] = df_test[\"datetime\"].dt.isocalendar().week.apply(lambda x: np.cos((x-1)*(2*np.pi/52)))\n",
    "\n",
    "x = np.arange(2, 8, 0.1)\n",
    "x = x-1\n",
    "y = np.sin((x-1)*(2*np.pi/6))\n",
    "y_cos = np.cos((x-1)*(2*np.pi/6))\n",
    "# Plotting Sine Graph\n",
    "plt.plot(x, y, color='green')\n",
    "plt.plot(x, y_cos, color=\"blue\")\n",
    "plt.scatter(np.array(df_train[\"day_of_week\"]), np.array(df_train[\"day_sin\"]))\n",
    "plt.scatter(np.array(df_train[\"day_of_week\"]), np.array(df_train[\"day_cos\"]))\n",
    "\n",
    "plt.title(\"Encoded day of the week values (sin+cos encoding)\")\n",
    "plt.show()\n",
    "\n",
    "x = np.arange(1, 52, 0.5)\n",
    "y = np.sin((x-1)*(2*np.pi/52))\n",
    "y_cos = np.cos((x-1)*(2*np.pi/52))\n",
    "\n",
    "plt.plot(x, y, color='green')\n",
    "plt.plot(x, y_cos, color=\"blue\")\n",
    "plt.scatter(np.array(df_train[\"datetime\"].dt.isocalendar().week), np.array(df_train[\"day_year_sin\"]))\n",
    "plt.scatter(np.array(df_train[\"datetime\"].dt.isocalendar().week), np.array(df_train[\"day_year_cos\"]))\n",
    "\n",
    "plt.scatter(np.array(df_test[\"datetime\"].dt.isocalendar().week), np.array(df_test[\"day_year_sin\"]), color=\"brown\")\n",
    "plt.scatter(np.array(df_test[\"datetime\"].dt.isocalendar().week), np.array(df_test[\"day_year_cos\"]), color=\"purple\")\n",
    "plt.title(\"Encoded week of the year values\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Windowing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original sample: 980, windowed data points: 765\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 16\u001b[0m\n\u001b[1;32m      3\u001b[0m w_mood_classification \u001b[38;5;241m=\u001b[39m WindowGenerator(\n\u001b[1;32m      4\u001b[0m   input_width\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m7\u001b[39m,\n\u001b[1;32m      5\u001b[0m   label_width\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     12\u001b[0m   datetime_label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdatetime\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     13\u001b[0m )\n\u001b[1;32m     15\u001b[0m X_train_classification, Y_train_classification \u001b[38;5;241m=\u001b[39m w_mood_classification\u001b[38;5;241m.\u001b[39mtrain\n\u001b[0;32m---> 16\u001b[0m X_validation_classification, Y_validation_classification \u001b[38;5;241m=\u001b[39m \u001b[43mw_mood_classification\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mval\u001b[49m\n\u001b[1;32m     17\u001b[0m X_test_classification, Y_test_classification \u001b[38;5;241m=\u001b[39m w_mood_classification\u001b[38;5;241m.\u001b[39mtest\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28mprint\u001b[39m(X_train_classification\u001b[38;5;241m.\u001b[39mshape, Y_train_classification\u001b[38;5;241m.\u001b[39mshape)\n",
      "File \u001b[0;32m~/Desktop/Projects/DMT/windowing.py:116\u001b[0m, in \u001b[0;36mWindowGenerator.val\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mval\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit_window\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdf_val\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Projects/DMT/windowing.py:64\u001b[0m, in \u001b[0;36mWindowGenerator.split_window\u001b[0;34m(self, df_data)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Given a dataframe, splits the dataframe into chunks, where one chunk\u001b[39;00m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;124;03mis a number of consecutive days. In case one day is missing in the sequence,\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;124;03mthis sequence is dropped. The result will be X data that is windowed, together\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;124;03m    and Y is an array with single values (target values).\u001b[39;00m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;66;03m# time column should be day date\u001b[39;00m\n\u001b[0;32m---> 64\u001b[0m df_data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdate\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdf_data\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdatetime_label\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mdt\u001b[38;5;241m.\u001b[39mdate\n\u001b[1;32m     65\u001b[0m df_data \u001b[38;5;241m=\u001b[39m df_data\u001b[38;5;241m.\u001b[39msort_values(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdate\u001b[39m\u001b[38;5;124m\"\u001b[39m, ascending\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     67\u001b[0m \u001b[38;5;66;03m#used_columns = [\"mood\",\"circumplex.arousal\",\"circumplex.valence\",\"activity\",\"screen\",\u001b[39;00m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;66;03m#\"call\",\"sms\",\"appCat_total\", \"day_sin\", \"day_cos\"]#df_data.select_dtypes(include=['float64', 'int64']).columns\u001b[39;00m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;66;03m#sed_columns = used_columns.drop(self.label_column).intersection(pd.Index(self.feature_columns))\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "from windowing import WindowGenerator\n",
    "\n",
    "w_mood_classification = WindowGenerator(\n",
    "  input_width=7,\n",
    "  label_width=0, \n",
    "  label_column=Y_features[0], \n",
    "  feature_columns=X_features,\n",
    "  df_train=df_train,\n",
    "  df_val=df_val,\n",
    "  df_test=df_test,\n",
    "  num_classes=10,\n",
    "  datetime_label=\"datetime\",\n",
    ")\n",
    "\n",
    "X_train_classification, Y_train_classification = w_mood_classification.train\n",
    "X_validation_classification, Y_validation_classification = w_mood_classification.val\n",
    "X_test_classification, Y_test_classification = w_mood_classification.test\n",
    "\n",
    "print(X_train_classification.shape, Y_train_classification.shape)\n",
    "print(X_validation_classification.shape, Y_validation_classification.shape)\n",
    "print(X_test_classification.shape, X_test_classification.shape)\n",
    "\n",
    "\n",
    "w_mood_regression = WindowGenerator(\n",
    "  input_width=7,\n",
    "  label_width=0, \n",
    "  label_column=Y_features[0], \n",
    "  feature_columns=X_features,\n",
    "  df_train=df_train,\n",
    "  df_val=df_val,\n",
    "  df_test=df_test,\n",
    "  num_classes=10,\n",
    "  datetime_label=\"datetime\",\n",
    "  regression=True\n",
    ")\n",
    "\n",
    "X_train_regression, Y_train_regression = w_mood_regression.train\n",
    "X_validation_regression, Y_validation_regression = w_mood_regression.val\n",
    "X_test_regression, Y_test_regression = w_mood_regression.test\n",
    "\n",
    "print(X_train_regression.shape, Y_train_regression.shape)\n",
    "print(X_validation_regression.shape, Y_validation_regression.shape)\n",
    "print(X_test_regression.shape, Y_test_regression.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we setup the code for optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shared variables in both optimization parts\n",
    "repetitions = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "cannot unpack non-iterable ellipsis object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dense, LSTM, Dropout\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Sequential\n\u001b[0;32m----> 5\u001b[0m X_train_classification, Y_train_classification \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m      6\u001b[0m X_validation_classification, Y_validation_classification \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# function that should be maximized by the bayesian optimization\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: cannot unpack non-iterable ellipsis object"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Dense, LSTM, Dropout\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "\n",
    "def get_classification_model(encoding_layers, classification_layers, lstm_units, dropout):\n",
    "    encoding_layers_list = []\n",
    "    for i in range(encoding_layers):\n",
    "        encoding_layers_list += [Dense(2**(5+i), activation=\"relu\"), Dropout(dropout)]\n",
    "\n",
    "    classification_layers_list = []\n",
    "    for i in range(classification_layers):\n",
    "        classification_layers_list += [Dropout(dropout), Dense(2**(5+i), activation=\"relu\")]\n",
    "    classification_layers_list = classification_layers_list[::-1]\n",
    "    classification_layers_list = classification_layers_list[:-1]\n",
    "    LSTM_model = Sequential([\n",
    "        *encoding_layers_list,\n",
    "        LSTM(lstm_units),\n",
    "        *classification_layers_list,\n",
    "        Dense(n_classes, activation=\"softmax\")\n",
    "    ])\n",
    "    return LSTM_model\n",
    "\n",
    "# function that should be maximized by the bayesian optimization\n",
    "def train_wrapper_classification(bs: float, lr: float, lstm_units: float, encoding_layers: float, classification_layers: float, dropout: float):\n",
    "    batch_size = int(bs)\n",
    "    lstm_units = int(lstm_units)\n",
    "    encoding_layers = int(encoding_layers)\n",
    "    classification_layers = int(classification_layers)\n",
    "\n",
    "    # return negative value to turn a minimization to a maximization problem\n",
    "    results = [\n",
    "        -train_network_classification(batch_size, lr, lstm_units, encoding_layers, classification_layers, dropout) for _ in range(repetitions)\n",
    "    ]\n",
    "    # average scores\n",
    "    return sum(results)/len(results)\n",
    "\n",
    "\n",
    "def train_network_classification(bs: int, lr: float, lstm_units: int, encoding_layers: int, classification_layers: int, dropout: float) -> float:\n",
    "    \n",
    "    #print(LSTM_model.summary())\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=150,\n",
    "        mode='min'\n",
    "    )\n",
    "    LSTM_model = get_classification_model(encoding_layers, classification_layers, lstm_units, dropout)\n",
    "    LSTM_model.compile(\n",
    "        loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=lr),\n",
    "        metrics=[\"accuracy\", \"f1_score\"]\n",
    "    )\n",
    "\n",
    "    history = LSTM_model.fit(x=X_train_classification, y=Y_train_classification, epochs=1000, batch_size=bs,\n",
    "                        validation_data=(X_validation_classification, Y_validation_classification),\n",
    "                        callbacks=[early_stopping],\n",
    "                        verbose=0)\n",
    "    # return the lowest validation loss seen\n",
    "    return min(history.history[\"val_loss\"])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the same for the regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense, LSTM, Dropout\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "\n",
    "def get_regression_model(encoding_layers, regression_layers, lstm_units, dropout):\n",
    "    encoding_layers_list = []\n",
    "    for i in range(encoding_layers):\n",
    "        encoding_layers_list += [Dense(2**(5+i), activation=\"relu\"), Dropout(dropout)]\n",
    "\n",
    "    regression_layers_list = []\n",
    "    for i in range(regression_layers):\n",
    "        regression_layers_list += [Dropout(dropout), Dense(2**(5+i), activation=\"relu\")]\n",
    "    regression_layers_list = regression_layers_list[::-1]\n",
    "    regression_layers_list = regression_layers_list[:-1]\n",
    "    LSTM_model = Sequential([\n",
    "        *encoding_layers_list,\n",
    "        LSTM(lstm_units),\n",
    "        *regression_layers_list,\n",
    "        Dense(1)\n",
    "    ])\n",
    "    return LSTM_model\n",
    "\n",
    "# function that should be maximized by the bayesian optimization\n",
    "def train_wrapper_regression(bs: float, lr: float, lstm_units: float, encoding_layers: float, classification_layers: float, dropout: float):\n",
    "    batch_size = int(bs)\n",
    "    lstm_units = int(lstm_units)\n",
    "    encoding_layers = int(encoding_layers)\n",
    "    classification_layers = int(classification_layers)\n",
    "\n",
    "    # return negative value to turn a minimization to a maximization problem\n",
    "    results = [\n",
    "        -train_network_regression(batch_size, lr, lstm_units, encoding_layers, classification_layers, dropout) for _ in range(repetitions)\n",
    "    ]\n",
    "    # average scores\n",
    "    return sum(results)/len(results)\n",
    "\n",
    "\n",
    "def train_network_regression(bs: int, lr: float, lstm_units: int, encoding_layers: int, regression_layers: int, dropout: float) -> float:\n",
    "    #print(LSTM_model.summary())\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=150,\n",
    "        mode='min'\n",
    "    )\n",
    "    LSTM_model = get_regression_model(encoding_layers, regression_layers, lstm_units, dropout)\n",
    "    LSTM_model.compile(\n",
    "        loss=tf.keras.losses.MeanSquaredError(),\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=lr),\n",
    "        metrics=[\"mse\", \"mae\"]\n",
    "    )\n",
    "\n",
    "    history = LSTM_model.fit(x=X_train_regression, y=Y_train_regression, epochs=1000, batch_size=bs,\n",
    "                        validation_data=(X_validation_regression, Y_validation_regression),\n",
    "                        callbacks=[early_stopping],\n",
    "                        verbose=0)\n",
    "    # return the lowest validation loss seen\n",
    "    return min(history.history[\"val_loss\"])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compile_and_fit(model, window, patience=30, bs=10, lr=0.01, max_epochs=30, regression=False):\n",
    "  early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss',\n",
    "                                                    patience=patience,\n",
    "                                                    mode='min')\n",
    "  if not regression:\n",
    "    model.compile(loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "                  optimizer=tf.keras.optimizers.Adam(learning_rate=lr),\n",
    "                  metrics=[\"accuracy\", \"f1_score\"])\n",
    "  else:\n",
    "    model.compile(loss=tf.keras.losses.MeanSquaredError(),\n",
    "                  optimizer=tf.keras.optimizers.Adam(learning_rate=lr),\n",
    "                  metrics=[\"mse\", \"mae\"])\n",
    "  X_train, Y_train = window.train\n",
    "  X_test, Y_test = window.test\n",
    "  history = model.fit(x=X_train, y=Y_train, epochs=max_epochs, batch_size=bs,\n",
    "                      validation_data=(X_test, Y_test),\n",
    "                      callbacks=[early_stopping])\n",
    "  return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install bayesian-optimization\n",
    "from bayes_opt import BayesianOptimization\n",
    "\n",
    "\n",
    "# Bounded region of parameter space\n",
    "pbounds = {\n",
    "    'bs': (1, 1000),\n",
    "    'lr': (0, 0.01),\n",
    "    \"lstm_units\": (32, 512),\n",
    "    \"encoding_layers\": (1, 4),\n",
    "    \"classification_layers\": (1, 4),\n",
    "    \"dropout\": (0, 0.6)\n",
    "}\n",
    "\n",
    "\n",
    "optimizer = BayesianOptimization(\n",
    "    f=train_wrapper_classification,\n",
    "    pbounds=pbounds,\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "optimizer.maximize(\n",
    "    init_points=2,\n",
    "    n_iter=50,\n",
    ")\n",
    "\n",
    "print(optimizer.max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original sample: 980, windowed data points: 765\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/m/Desktop/Projects/DMT/.venv/lib/python3.11/site-packages/keras/src/layers/core/dense.py:86: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original sample: 980, windowed data points: 765\n",
      "Original sample: 261, windowed data points: 87\n",
      "Epoch 1/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4s/step - accuracy: 0.1529 - f1_score: 0.0631 - loss: 2.4883 - val_accuracy: 0.3448 - val_f1_score: 0.0885 - val_loss: 2.2494\n",
      "Epoch 2/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 102ms/step - accuracy: 0.1529 - f1_score: 0.0588 - loss: 2.4882 - val_accuracy: 0.3333 - val_f1_score: 0.0839 - val_loss: 2.2447\n",
      "Epoch 3/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104ms/step - accuracy: 0.1359 - f1_score: 0.0581 - loss: 2.4818 - val_accuracy: 0.3333 - val_f1_score: 0.0821 - val_loss: 2.2401\n",
      "Epoch 4/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 99ms/step - accuracy: 0.1595 - f1_score: 0.0678 - loss: 2.3906 - val_accuracy: 0.3218 - val_f1_score: 0.0780 - val_loss: 2.2346\n",
      "Epoch 5/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 100ms/step - accuracy: 0.1791 - f1_score: 0.0710 - loss: 2.3662 - val_accuracy: 0.3563 - val_f1_score: 0.0907 - val_loss: 2.2287\n",
      "Epoch 6/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 96ms/step - accuracy: 0.1686 - f1_score: 0.0656 - loss: 2.3800 - val_accuracy: 0.4023 - val_f1_score: 0.1034 - val_loss: 2.2226\n",
      "Epoch 7/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 110ms/step - accuracy: 0.1673 - f1_score: 0.0811 - loss: 2.4023 - val_accuracy: 0.4483 - val_f1_score: 0.1234 - val_loss: 2.2167\n",
      "Epoch 8/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 97ms/step - accuracy: 0.1686 - f1_score: 0.0666 - loss: 2.3733 - val_accuracy: 0.4368 - val_f1_score: 0.1283 - val_loss: 2.2108\n",
      "Epoch 9/3000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 28\u001b[0m\n\u001b[1;32m     11\u001b[0m LSTM_model \u001b[38;5;241m=\u001b[39m Sequential([\n\u001b[1;32m     12\u001b[0m     Dense(\u001b[38;5;241m16\u001b[39m, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m\"\u001b[39m, input_shape\u001b[38;5;241m=\u001b[39m(x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m])),\n\u001b[1;32m     13\u001b[0m     BatchNormalization(),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     24\u001b[0m     Dense(n_classes, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msoftmax\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     25\u001b[0m ])\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m#LSTM_model.summary()\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m \u001b[43mcompile_and_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mLSTM_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mw_mood\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpatience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mMAX_EPOCHS\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[29], line 15\u001b[0m, in \u001b[0;36mcompile_and_fit\u001b[0;34m(model, window, patience, bs, lr, max_epochs, regression)\u001b[0m\n\u001b[1;32m     13\u001b[0m X_train, Y_train \u001b[38;5;241m=\u001b[39m window\u001b[38;5;241m.\u001b[39mtrain\n\u001b[1;32m     14\u001b[0m X_test, Y_test \u001b[38;5;241m=\u001b[39m window\u001b[38;5;241m.\u001b[39mtest\n\u001b[0;32m---> 15\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mY_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_test\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mearly_stopping\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m history\n",
      "File \u001b[0;32m~/Desktop/Projects/DMT/.venv/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/Desktop/Projects/DMT/.venv/lib/python3.11/site-packages/keras/src/backend/tensorflow/trainer.py:329\u001b[0m, in \u001b[0;36mTensorFlowTrainer.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[1;32m    327\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator\u001b[38;5;241m.\u001b[39menumerate_epoch():\n\u001b[1;32m    328\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m--> 329\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    330\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_end(\n\u001b[1;32m    331\u001b[0m         step, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pythonify_logs(logs)\n\u001b[1;32m    332\u001b[0m     )\n\u001b[1;32m    333\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop_training:\n",
      "File \u001b[0;32m~/Desktop/Projects/DMT/.venv/lib/python3.11/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/Desktop/Projects/DMT/.venv/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:833\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    830\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 833\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    835\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    836\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/Desktop/Projects/DMT/.venv/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:878\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    875\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    876\u001b[0m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[1;32m    877\u001b[0m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[0;32m--> 878\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mtracing_compilation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_variable_creation_config\u001b[49m\n\u001b[1;32m    880\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    881\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_created_variables:\n\u001b[1;32m    882\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating variables on a non-first call to a function\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    883\u001b[0m                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m decorated with tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Desktop/Projects/DMT/.venv/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[0;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[1;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Projects/DMT/.venv/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py:1322\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1318\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1320\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1321\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1322\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1323\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1324\u001b[0m     args,\n\u001b[1;32m   1325\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1326\u001b[0m     executing_eagerly)\n\u001b[1;32m   1327\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/Desktop/Projects/DMT/.venv/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[0;32m~/Desktop/Projects/DMT/.venv/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[1;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[1;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[1;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[1;32m    261\u001b[0m     )\n",
      "File \u001b[0;32m~/Desktop/Projects/DMT/.venv/lib/python3.11/site-packages/tensorflow/python/eager/context.py:1500\u001b[0m, in \u001b[0;36mContext.call_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1498\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[1;32m   1499\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1500\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1501\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1503\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1504\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1505\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1506\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1507\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1508\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m   1509\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1510\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1514\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[1;32m   1515\u001b[0m   )\n",
      "File \u001b[0;32m~/Desktop/Projects/DMT/.venv/lib/python3.11/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Dense, LSTM, BatchNormalization, Dropout\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "n_classes = 10\n",
    "MAX_EPOCHS = 3000\n",
    "lr = 0.0002\n",
    "bs = 1000\n",
    "\n",
    "\n",
    "x = w_mood.train[0]\n",
    "LSTM_model = Sequential([\n",
    "    Dense(16, activation=\"relu\", input_shape=(x.shape[1], x.shape[2])),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.5),\n",
    "    Dense(32, activation=\"tanh\"),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.5),\n",
    "    Dense(128, activation=\"tanh\"),\n",
    "    LSTM(128, dropout=0.2),\n",
    "    Dense(64, activation=\"relu\"),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.5),\n",
    "    Dense(32, activation=\"relu\"),\n",
    "    Dense(n_classes, activation=\"softmax\")\n",
    "])\n",
    "#LSTM_model.summary()\n",
    "\n",
    "compile_and_fit(LSTM_model, w_mood, patience=500, bs=bs, lr=lr, max_epochs=MAX_EPOCHS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original sample: 980, windowed data points: 765\n",
      "Original sample: 261, windowed data points: 87\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step\n",
      "0.7398692810457517\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "0.6551724137931034\n"
     ]
    }
   ],
   "source": [
    "# evaluate final model\n",
    "x, y = w_mood.train\n",
    "x_test, y_test = w_mood.test\n",
    "y_hat = LSTM_model.predict(x)\n",
    "y_hat_index = np.argmax(y_hat, axis=1)\n",
    "y_actual_train = np.argmax(y, axis=1)\n",
    "print(sum(y_hat_index==y_actual_train)/y_actual_train.shape[0])\n",
    "\n",
    "y_hat = LSTM_model.predict(x_test)\n",
    "y_hat_index = np.argmax(y_hat, axis=1)\n",
    "y_actual = np.argmax(y_test, axis=1)\n",
    "print(sum(y_hat_index==y_actual)/y_actual.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plotting mood categories\n",
    "y_fixed_actual = y_actual_train + 1\n",
    "y_number = sum(((y_fixed_actual>4)) & (y_fixed_actual<=7))\n",
    "print(y_number)\n",
    "\n",
    "barlist = plt.bar([2, 6, 9], [sum(y_fixed_actual<=4), y_number, sum(y_fixed_actual>7)], width=2.9)\n",
    "barlist[0].set_width(3.9)\n",
    "plt.hist(y_fixed_actual, color=\"red\", alpha=0.8)\n",
    "plt.text(1.3, 20, \"low (<=4)\")\n",
    "plt.text(5, 500, \"    medium\\n ($>4$ & $\\leq7$)\")\n",
    "plt.text(8.3, 200, \"high (>7)\")\n",
    "plt.title(\"Class Histogram\")\n",
    "plt.savefig(\"images/rnn_dataset\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original sample: 980, windowed data points: 765\n",
      "Training data shape: (765, 7, 23), y shape: (765,)\n",
      "['mood', 'circumplex.arousal', 'circumplex.valence', 'activity', 'screen', 'call', 'sms', 'day_sin', 'day_cos', 'day_year_sin', 'day_year_cos', 'appCat_total', 'appCat.builtin', 'appCat.communication', 'appCat.entertainment', 'appCat.finance', 'appCat.game', 'appCat.office', 'appCat.social', 'appCat.travel', 'appCat.unknown', 'appCat.utilities', 'appCat.weather']\n",
      "[ 6.          0.55        0.55        0.1340501   2.52503301  1.08778607\n",
      " -0.47901595 -0.8660254  -0.5         0.97094182  0.23931566  1.81896206\n",
      "  0.54173338  1.48082884  0.08555424  0.66569183 -0.16014185  0.06350993\n",
      "  1.74995446  2.27722089 -0.20724835  3.55248874 -0.20165322]\n",
      "6.0\n"
     ]
    }
   ],
   "source": [
    "w_mood_regression = WindowGenerator(7, 0, \n",
    "  label_column=Y_features[0], \n",
    "  feature_columns=X_features,\n",
    "  df_train=df_train,\n",
    "  df_val=None,\n",
    "  df_test=df_test,\n",
    "  num_classes=10,\n",
    "  datetime_label=\"datetime\",\n",
    "  regression=True\n",
    ")\n",
    "X_train_windowed, Y_train = w_mood_regression.train\n",
    "print(f\"Training data shape: {X_train_windowed.shape}, y shape: {Y_train.shape}\")\n",
    "print(w_mood_regression.feature_columns)\n",
    "print(X_train_windowed[0][0])\n",
    "print(Y_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original sample: 980, windowed data points: 765\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/m/Desktop/Projects/DMT/.venv/lib/python3.11/site-packages/keras/src/layers/core/dense.py:86: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original sample: 980, windowed data points: 765\n",
      "Original sample: 261, windowed data points: 87\n",
      "Epoch 1/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - loss: 53.4566 - mae: 7.2616 - mse: 53.4566 - val_loss: 53.9217 - val_mae: 7.3000 - val_mse: 53.9217\n",
      "Epoch 2/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step - loss: 51.9665 - mae: 7.1590 - mse: 51.9665 - val_loss: 52.3876 - val_mae: 7.1946 - val_mse: 52.3876\n",
      "Epoch 3/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - loss: 50.4791 - mae: 7.0543 - mse: 50.4791 - val_loss: 50.8861 - val_mae: 7.0899 - val_mse: 50.8861\n",
      "Epoch 4/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - loss: 48.8884 - mae: 6.9421 - mse: 48.8884 - val_loss: 49.4395 - val_mae: 6.9876 - val_mse: 49.4395\n",
      "Epoch 5/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step - loss: 47.6241 - mae: 6.8506 - mse: 47.6241 - val_loss: 48.0436 - val_mae: 6.8874 - val_mse: 48.0436\n",
      "Epoch 6/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - loss: 46.2795 - mae: 6.7519 - mse: 46.2795 - val_loss: 46.7193 - val_mae: 6.7910 - val_mse: 46.7193\n",
      "Epoch 7/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step - loss: 44.9617 - mae: 6.6534 - mse: 44.9617 - val_loss: 45.4562 - val_mae: 6.6977 - val_mse: 45.4562\n",
      "Epoch 8/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - loss: 43.7963 - mae: 6.5660 - mse: 43.7963 - val_loss: 44.2229 - val_mae: 6.6054 - val_mse: 44.2229\n",
      "Epoch 9/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step - loss: 42.4262 - mae: 6.4614 - mse: 42.4262 - val_loss: 42.9982 - val_mae: 6.5124 - val_mse: 42.9982\n",
      "Epoch 10/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - loss: 41.2616 - mae: 6.3702 - mse: 41.2616 - val_loss: 41.7916 - val_mae: 6.4194 - val_mse: 41.7916\n",
      "Epoch 11/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - loss: 40.0515 - mae: 6.2756 - mse: 40.0515 - val_loss: 40.6111 - val_mae: 6.3270 - val_mse: 40.6111\n",
      "Epoch 12/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - loss: 38.8472 - mae: 6.1786 - mse: 38.8472 - val_loss: 39.4544 - val_mae: 6.2353 - val_mse: 39.4544\n",
      "Epoch 13/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - loss: 37.8490 - mae: 6.0973 - mse: 37.8490 - val_loss: 38.3315 - val_mae: 6.1450 - val_mse: 38.3315\n",
      "Epoch 14/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - loss: 36.6748 - mae: 6.0000 - mse: 36.6748 - val_loss: 37.2183 - val_mae: 6.0542 - val_mse: 37.2183\n",
      "Epoch 15/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 35.6084 - mae: 5.9109 - mse: 35.6084 - val_loss: 36.0984 - val_mae: 5.9614 - val_mse: 36.0984\n",
      "Epoch 16/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 34.3641 - mae: 5.8049 - mse: 34.3641 - val_loss: 34.9703 - val_mae: 5.8663 - val_mse: 34.9703\n",
      "Epoch 17/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - loss: 33.2522 - mae: 5.7089 - mse: 33.2522 - val_loss: 33.8374 - val_mae: 5.7693 - val_mse: 33.8374\n",
      "Epoch 18/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - loss: 32.1186 - mae: 5.6077 - mse: 32.1186 - val_loss: 32.7037 - val_mae: 5.6704 - val_mse: 32.7037\n",
      "Epoch 19/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step - loss: 30.9799 - mae: 5.5057 - mse: 30.9799 - val_loss: 31.5682 - val_mae: 5.5697 - val_mse: 31.5682\n",
      "Epoch 20/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 93ms/step - loss: 29.7486 - mae: 5.3923 - mse: 29.7486 - val_loss: 30.4237 - val_mae: 5.4662 - val_mse: 30.4237\n",
      "Epoch 21/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 28.6906 - mae: 5.2936 - mse: 28.6906 - val_loss: 29.2706 - val_mae: 5.3600 - val_mse: 29.2706\n",
      "Epoch 22/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - loss: 27.3990 - mae: 5.1689 - mse: 27.3990 - val_loss: 28.1047 - val_mae: 5.2503 - val_mse: 28.1047\n",
      "Epoch 23/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - loss: 26.4130 - mae: 5.0719 - mse: 26.4130 - val_loss: 26.9261 - val_mae: 5.1370 - val_mse: 26.9261\n",
      "Epoch 24/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 25.1896 - mae: 4.9518 - mse: 25.1896 - val_loss: 25.7339 - val_mae: 5.0198 - val_mse: 25.7339\n",
      "Epoch 25/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - loss: 23.9026 - mae: 4.8202 - mse: 23.9026 - val_loss: 24.5300 - val_mae: 4.8985 - val_mse: 24.5300\n",
      "Epoch 26/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step - loss: 22.7502 - mae: 4.6963 - mse: 22.7502 - val_loss: 23.3158 - val_mae: 4.7731 - val_mse: 23.3158\n",
      "Epoch 27/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 96ms/step - loss: 21.3908 - mae: 4.5495 - mse: 21.3908 - val_loss: 22.0922 - val_mae: 4.6433 - val_mse: 22.0922\n",
      "Epoch 28/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - loss: 20.4492 - mae: 4.4452 - mse: 20.4492 - val_loss: 20.8621 - val_mae: 4.5090 - val_mse: 20.8621\n",
      "Epoch 29/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - loss: 19.1899 - mae: 4.3033 - mse: 19.1899 - val_loss: 19.6258 - val_mae: 4.3699 - val_mse: 19.6258\n",
      "Epoch 30/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 17.8399 - mae: 4.1408 - mse: 17.8399 - val_loss: 18.3867 - val_mae: 4.2259 - val_mse: 18.3867\n",
      "Epoch 31/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 16.7439 - mae: 4.0041 - mse: 16.7439 - val_loss: 17.1478 - val_mae: 4.0767 - val_mse: 17.1478\n",
      "Epoch 32/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - loss: 15.5661 - mae: 3.8586 - mse: 15.5661 - val_loss: 15.9136 - val_mae: 3.9225 - val_mse: 15.9136\n",
      "Epoch 33/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - loss: 14.2811 - mae: 3.6866 - mse: 14.2811 - val_loss: 14.6821 - val_mae: 3.7622 - val_mse: 14.6821\n",
      "Epoch 34/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - loss: 13.1898 - mae: 3.5366 - mse: 13.1898 - val_loss: 13.4536 - val_mae: 3.5953 - val_mse: 13.4536\n",
      "Epoch 35/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - loss: 11.9193 - mae: 3.3501 - mse: 11.9193 - val_loss: 12.2481 - val_mae: 3.4236 - val_mse: 12.2481\n",
      "Epoch 36/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - loss: 10.8336 - mae: 3.1826 - mse: 10.8336 - val_loss: 11.0741 - val_mae: 3.2477 - val_mse: 11.0741\n",
      "Epoch 37/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - loss: 9.8661 - mae: 3.0237 - mse: 9.8661 - val_loss: 9.9387 - val_mae: 3.0680 - val_mse: 9.9387\n",
      "Epoch 38/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - loss: 8.6581 - mae: 2.8240 - mse: 8.6581 - val_loss: 8.8506 - val_mae: 2.8852 - val_mse: 8.8506\n",
      "Epoch 39/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - loss: 7.8385 - mae: 2.6818 - mse: 7.8385 - val_loss: 7.8153 - val_mae: 2.6998 - val_mse: 7.8153\n",
      "Epoch 40/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - loss: 6.8310 - mae: 2.4892 - mse: 6.8310 - val_loss: 6.8390 - val_mae: 2.5124 - val_mse: 6.8390\n",
      "Epoch 41/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - loss: 5.9006 - mae: 2.2954 - mse: 5.9006 - val_loss: 5.9268 - val_mae: 2.3237 - val_mse: 5.9268\n",
      "Epoch 42/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - loss: 5.0198 - mae: 2.0933 - mse: 5.0198 - val_loss: 5.0837 - val_mae: 2.1346 - val_mse: 5.0837\n",
      "Epoch 43/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step - loss: 4.5022 - mae: 1.9512 - mse: 4.5022 - val_loss: 4.3127 - val_mae: 1.9458 - val_mse: 4.3127\n",
      "Epoch 44/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - loss: 3.7250 - mae: 1.7532 - mse: 3.7250 - val_loss: 3.6167 - val_mae: 1.7621 - val_mse: 3.6167\n",
      "Epoch 45/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step - loss: 3.2271 - mae: 1.6158 - mse: 3.2271 - val_loss: 2.9967 - val_mae: 1.5809 - val_mse: 2.9967\n",
      "Epoch 46/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 2.6743 - mae: 1.4408 - mse: 2.6743 - val_loss: 2.4534 - val_mae: 1.4056 - val_mse: 2.4534\n",
      "Epoch 47/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - loss: 2.2241 - mae: 1.2778 - mse: 2.2241 - val_loss: 1.9864 - val_mae: 1.2450 - val_mse: 1.9864\n",
      "Epoch 48/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - loss: 1.7572 - mae: 1.1120 - mse: 1.7572 - val_loss: 1.5943 - val_mae: 1.0952 - val_mse: 1.5943\n",
      "Epoch 49/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - loss: 1.4476 - mae: 0.9872 - mse: 1.4476 - val_loss: 1.2737 - val_mae: 0.9658 - val_mse: 1.2737\n",
      "Epoch 50/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - loss: 1.1429 - mae: 0.8661 - mse: 1.1429 - val_loss: 1.0200 - val_mae: 0.8464 - val_mse: 1.0200\n",
      "Epoch 51/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - loss: 0.9802 - mae: 0.7848 - mse: 0.9802 - val_loss: 0.8277 - val_mae: 0.7383 - val_mse: 0.8277\n",
      "Epoch 52/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - loss: 0.9650 - mae: 0.7836 - mse: 0.9650 - val_loss: 0.6899 - val_mae: 0.6458 - val_mse: 0.6899\n",
      "Epoch 53/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 0.8184 - mae: 0.7108 - mse: 0.8184 - val_loss: 0.5990 - val_mae: 0.5683 - val_mse: 0.5990\n",
      "Epoch 54/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - loss: 0.8362 - mae: 0.7199 - mse: 0.8362 - val_loss: 0.5478 - val_mae: 0.5255 - val_mse: 0.5478\n",
      "Epoch 55/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 0.8531 - mae: 0.7156 - mse: 0.8531 - val_loss: 0.5281 - val_mae: 0.5306 - val_mse: 0.5281\n",
      "Epoch 56/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - loss: 0.8401 - mae: 0.7215 - mse: 0.8401 - val_loss: 0.5313 - val_mae: 0.5492 - val_mse: 0.5313\n",
      "Epoch 57/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 0.9106 - mae: 0.7403 - mse: 0.9106 - val_loss: 0.5497 - val_mae: 0.5756 - val_mse: 0.5497\n",
      "Epoch 58/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - loss: 1.0427 - mae: 0.8015 - mse: 1.0427 - val_loss: 0.5761 - val_mae: 0.6015 - val_mse: 0.5761\n",
      "Epoch 59/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - loss: 1.0498 - mae: 0.8072 - mse: 1.0498 - val_loss: 0.6045 - val_mae: 0.6255 - val_mse: 0.6045\n",
      "Epoch 60/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - loss: 1.1322 - mae: 0.8346 - mse: 1.1322 - val_loss: 0.6300 - val_mae: 0.6437 - val_mse: 0.6300\n",
      "Epoch 61/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - loss: 1.1265 - mae: 0.8302 - mse: 1.1265 - val_loss: 0.6496 - val_mae: 0.6563 - val_mse: 0.6496\n",
      "Epoch 62/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - loss: 1.1847 - mae: 0.8640 - mse: 1.1847 - val_loss: 0.6618 - val_mae: 0.6637 - val_mse: 0.6618\n",
      "Epoch 63/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - loss: 1.2472 - mae: 0.8809 - mse: 1.2472 - val_loss: 0.6654 - val_mae: 0.6660 - val_mse: 0.6654\n",
      "Epoch 64/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - loss: 1.2045 - mae: 0.8597 - mse: 1.2045 - val_loss: 0.6613 - val_mae: 0.6638 - val_mse: 0.6613\n",
      "Epoch 65/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step - loss: 1.2563 - mae: 0.8849 - mse: 1.2563 - val_loss: 0.6507 - val_mae: 0.6577 - val_mse: 0.6507\n",
      "Epoch 66/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - loss: 1.2130 - mae: 0.8713 - mse: 1.2130 - val_loss: 0.6350 - val_mae: 0.6482 - val_mse: 0.6350\n",
      "Epoch 67/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - loss: 1.1934 - mae: 0.8480 - mse: 1.1934 - val_loss: 0.6161 - val_mae: 0.6357 - val_mse: 0.6161\n",
      "Epoch 68/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 1.1592 - mae: 0.8434 - mse: 1.1592 - val_loss: 0.5962 - val_mae: 0.6209 - val_mse: 0.5962\n",
      "Epoch 69/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - loss: 1.1087 - mae: 0.8211 - mse: 1.1087 - val_loss: 0.5766 - val_mae: 0.6048 - val_mse: 0.5766\n",
      "Epoch 70/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - loss: 1.0840 - mae: 0.8233 - mse: 1.0840 - val_loss: 0.5589 - val_mae: 0.5883 - val_mse: 0.5589\n",
      "Epoch 71/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - loss: 0.9857 - mae: 0.7827 - mse: 0.9857 - val_loss: 0.5442 - val_mae: 0.5725 - val_mse: 0.5442\n",
      "Epoch 72/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 0.9499 - mae: 0.7694 - mse: 0.9499 - val_loss: 0.5333 - val_mae: 0.5583 - val_mse: 0.5333\n",
      "Epoch 73/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - loss: 0.9017 - mae: 0.7492 - mse: 0.9017 - val_loss: 0.5265 - val_mae: 0.5455 - val_mse: 0.5265\n",
      "Epoch 74/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - loss: 0.9412 - mae: 0.7540 - mse: 0.9412 - val_loss: 0.5238 - val_mae: 0.5353 - val_mse: 0.5238\n",
      "Epoch 75/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - loss: 0.8488 - mae: 0.7235 - mse: 0.8488 - val_loss: 0.5249 - val_mae: 0.5284 - val_mse: 0.5249\n",
      "Epoch 76/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - loss: 0.8323 - mae: 0.6975 - mse: 0.8323 - val_loss: 0.5293 - val_mae: 0.5241 - val_mse: 0.5293\n",
      "Epoch 77/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - loss: 0.8111 - mae: 0.7059 - mse: 0.8111 - val_loss: 0.5368 - val_mae: 0.5221 - val_mse: 0.5368\n",
      "Epoch 78/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - loss: 0.7867 - mae: 0.6880 - mse: 0.7867 - val_loss: 0.5466 - val_mae: 0.5254 - val_mse: 0.5466\n",
      "Epoch 79/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 0.8156 - mae: 0.7027 - mse: 0.8156 - val_loss: 0.5583 - val_mae: 0.5357 - val_mse: 0.5583\n",
      "Epoch 80/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - loss: 0.7841 - mae: 0.6826 - mse: 0.7841 - val_loss: 0.5711 - val_mae: 0.5476 - val_mse: 0.5711\n",
      "Epoch 81/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - loss: 0.8221 - mae: 0.7093 - mse: 0.8221 - val_loss: 0.5844 - val_mae: 0.5602 - val_mse: 0.5844\n",
      "Epoch 82/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - loss: 0.8493 - mae: 0.7154 - mse: 0.8493 - val_loss: 0.5978 - val_mae: 0.5732 - val_mse: 0.5978\n",
      "Epoch 83/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - loss: 0.8500 - mae: 0.7198 - mse: 0.8500 - val_loss: 0.6108 - val_mae: 0.5864 - val_mse: 0.6108\n",
      "Epoch 84/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - loss: 0.8334 - mae: 0.7052 - mse: 0.8334 - val_loss: 0.6229 - val_mae: 0.5981 - val_mse: 0.6229\n",
      "Epoch 85/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step - loss: 0.8230 - mae: 0.7086 - mse: 0.8230 - val_loss: 0.6336 - val_mae: 0.6079 - val_mse: 0.6336\n",
      "Epoch 86/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - loss: 0.8392 - mae: 0.7094 - mse: 0.8392 - val_loss: 0.6425 - val_mae: 0.6157 - val_mse: 0.6425\n",
      "Epoch 87/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - loss: 0.8226 - mae: 0.7201 - mse: 0.8226 - val_loss: 0.6500 - val_mae: 0.6223 - val_mse: 0.6500\n",
      "Epoch 88/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - loss: 0.8492 - mae: 0.7120 - mse: 0.8492 - val_loss: 0.6557 - val_mae: 0.6272 - val_mse: 0.6557\n",
      "Epoch 89/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - loss: 0.9309 - mae: 0.7491 - mse: 0.9309 - val_loss: 0.6596 - val_mae: 0.6306 - val_mse: 0.6596\n",
      "Epoch 90/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 0.8889 - mae: 0.7311 - mse: 0.8889 - val_loss: 0.6617 - val_mae: 0.6325 - val_mse: 0.6617\n",
      "Epoch 91/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 0.8438 - mae: 0.7256 - mse: 0.8438 - val_loss: 0.6621 - val_mae: 0.6329 - val_mse: 0.6621\n",
      "Epoch 92/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - loss: 0.8241 - mae: 0.7150 - mse: 0.8241 - val_loss: 0.6607 - val_mae: 0.6319 - val_mse: 0.6607\n",
      "Epoch 93/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - loss: 0.8401 - mae: 0.7267 - mse: 0.8401 - val_loss: 0.6578 - val_mae: 0.6298 - val_mse: 0.6578\n",
      "Epoch 94/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - loss: 0.8510 - mae: 0.7186 - mse: 0.8510 - val_loss: 0.6540 - val_mae: 0.6267 - val_mse: 0.6540\n",
      "Epoch 95/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - loss: 0.8799 - mae: 0.7312 - mse: 0.8799 - val_loss: 0.6489 - val_mae: 0.6226 - val_mse: 0.6489\n",
      "Epoch 96/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step - loss: 0.8359 - mae: 0.7216 - mse: 0.8359 - val_loss: 0.6432 - val_mae: 0.6178 - val_mse: 0.6432\n",
      "Epoch 97/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - loss: 0.8814 - mae: 0.7377 - mse: 0.8814 - val_loss: 0.6367 - val_mae: 0.6124 - val_mse: 0.6367\n",
      "Epoch 98/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - loss: 0.8587 - mae: 0.7431 - mse: 0.8587 - val_loss: 0.6297 - val_mae: 0.6065 - val_mse: 0.6297\n",
      "Epoch 99/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 0.8417 - mae: 0.7213 - mse: 0.8417 - val_loss: 0.6224 - val_mae: 0.6000 - val_mse: 0.6224\n",
      "Epoch 100/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - loss: 0.7863 - mae: 0.6907 - mse: 0.7863 - val_loss: 0.6152 - val_mae: 0.5935 - val_mse: 0.6152\n",
      "Epoch 101/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - loss: 0.8604 - mae: 0.7214 - mse: 0.8604 - val_loss: 0.6083 - val_mae: 0.5869 - val_mse: 0.6083\n",
      "Epoch 102/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - loss: 0.8597 - mae: 0.7332 - mse: 0.8597 - val_loss: 0.6013 - val_mae: 0.5803 - val_mse: 0.6013\n",
      "Epoch 103/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step - loss: 0.8309 - mae: 0.7247 - mse: 0.8309 - val_loss: 0.5946 - val_mae: 0.5737 - val_mse: 0.5946\n",
      "Epoch 104/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - loss: 0.8132 - mae: 0.6967 - mse: 0.8132 - val_loss: 0.5884 - val_mae: 0.5679 - val_mse: 0.5884\n",
      "Epoch 105/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - loss: 0.8627 - mae: 0.7120 - mse: 0.8627 - val_loss: 0.5829 - val_mae: 0.5628 - val_mse: 0.5829\n",
      "Epoch 106/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - loss: 0.8791 - mae: 0.7271 - mse: 0.8791 - val_loss: 0.5777 - val_mae: 0.5577 - val_mse: 0.5777\n",
      "Epoch 107/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - loss: 0.8373 - mae: 0.7066 - mse: 0.8373 - val_loss: 0.5732 - val_mae: 0.5533 - val_mse: 0.5732\n",
      "Epoch 108/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - loss: 0.8767 - mae: 0.7404 - mse: 0.8767 - val_loss: 0.5694 - val_mae: 0.5496 - val_mse: 0.5694\n",
      "Epoch 109/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - loss: 0.8392 - mae: 0.7148 - mse: 0.8392 - val_loss: 0.5659 - val_mae: 0.5463 - val_mse: 0.5659\n",
      "Epoch 110/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - loss: 0.8198 - mae: 0.7089 - mse: 0.8198 - val_loss: 0.5629 - val_mae: 0.5435 - val_mse: 0.5629\n",
      "Epoch 111/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - loss: 0.8469 - mae: 0.7175 - mse: 0.8469 - val_loss: 0.5603 - val_mae: 0.5411 - val_mse: 0.5603\n",
      "Epoch 112/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - loss: 0.8315 - mae: 0.7172 - mse: 0.8315 - val_loss: 0.5582 - val_mae: 0.5392 - val_mse: 0.5582\n",
      "Epoch 113/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - loss: 0.8922 - mae: 0.7338 - mse: 0.8922 - val_loss: 0.5567 - val_mae: 0.5378 - val_mse: 0.5567\n",
      "Epoch 114/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - loss: 0.8147 - mae: 0.7134 - mse: 0.8147 - val_loss: 0.5555 - val_mae: 0.5368 - val_mse: 0.5555\n",
      "Epoch 115/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - loss: 0.8709 - mae: 0.7277 - mse: 0.8709 - val_loss: 0.5549 - val_mae: 0.5363 - val_mse: 0.5549\n",
      "Epoch 116/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - loss: 0.8101 - mae: 0.7122 - mse: 0.8101 - val_loss: 0.5548 - val_mae: 0.5363 - val_mse: 0.5548\n",
      "Epoch 117/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - loss: 0.8535 - mae: 0.7137 - mse: 0.8535 - val_loss: 0.5550 - val_mae: 0.5366 - val_mse: 0.5550\n",
      "Epoch 118/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 0.8514 - mae: 0.7084 - mse: 0.8514 - val_loss: 0.5557 - val_mae: 0.5374 - val_mse: 0.5557\n",
      "Epoch 119/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - loss: 0.8216 - mae: 0.7091 - mse: 0.8216 - val_loss: 0.5566 - val_mae: 0.5383 - val_mse: 0.5566\n",
      "Epoch 120/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - loss: 0.7970 - mae: 0.6917 - mse: 0.7970 - val_loss: 0.5577 - val_mae: 0.5395 - val_mse: 0.5577\n",
      "Epoch 121/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - loss: 0.8788 - mae: 0.7249 - mse: 0.8788 - val_loss: 0.5591 - val_mae: 0.5409 - val_mse: 0.5591\n",
      "Epoch 122/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step - loss: 0.8205 - mae: 0.7072 - mse: 0.8205 - val_loss: 0.5607 - val_mae: 0.5426 - val_mse: 0.5607\n",
      "Epoch 123/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - loss: 0.8303 - mae: 0.7169 - mse: 0.8303 - val_loss: 0.5627 - val_mae: 0.5448 - val_mse: 0.5627\n",
      "Epoch 124/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - loss: 0.8318 - mae: 0.7135 - mse: 0.8318 - val_loss: 0.5650 - val_mae: 0.5472 - val_mse: 0.5650\n",
      "Epoch 125/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step - loss: 0.8010 - mae: 0.6789 - mse: 0.8010 - val_loss: 0.5673 - val_mae: 0.5497 - val_mse: 0.5673\n",
      "Epoch 126/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step - loss: 0.8337 - mae: 0.7152 - mse: 0.8337 - val_loss: 0.5696 - val_mae: 0.5523 - val_mse: 0.5696\n",
      "Epoch 127/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - loss: 0.8153 - mae: 0.6981 - mse: 0.8153 - val_loss: 0.5721 - val_mae: 0.5550 - val_mse: 0.5721\n",
      "Epoch 128/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - loss: 0.8283 - mae: 0.7111 - mse: 0.8283 - val_loss: 0.5747 - val_mae: 0.5579 - val_mse: 0.5747\n",
      "Epoch 129/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - loss: 0.8547 - mae: 0.7251 - mse: 0.8547 - val_loss: 0.5775 - val_mae: 0.5609 - val_mse: 0.5775\n",
      "Epoch 130/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - loss: 0.8322 - mae: 0.7174 - mse: 0.8322 - val_loss: 0.5803 - val_mae: 0.5638 - val_mse: 0.5803\n",
      "Epoch 131/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 0.8696 - mae: 0.7254 - mse: 0.8696 - val_loss: 0.5831 - val_mae: 0.5666 - val_mse: 0.5831\n",
      "Epoch 132/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - loss: 0.8510 - mae: 0.7240 - mse: 0.8510 - val_loss: 0.5854 - val_mae: 0.5690 - val_mse: 0.5854\n",
      "Epoch 133/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 102ms/step - loss: 0.8430 - mae: 0.7208 - mse: 0.8430 - val_loss: 0.5875 - val_mae: 0.5711 - val_mse: 0.5875\n",
      "Epoch 134/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 0.7867 - mae: 0.7023 - mse: 0.7867 - val_loss: 0.5892 - val_mae: 0.5731 - val_mse: 0.5892\n",
      "Epoch 135/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - loss: 0.8227 - mae: 0.6970 - mse: 0.8227 - val_loss: 0.5909 - val_mae: 0.5749 - val_mse: 0.5909\n",
      "Epoch 136/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 0.8482 - mae: 0.7141 - mse: 0.8482 - val_loss: 0.5921 - val_mae: 0.5763 - val_mse: 0.5921\n",
      "Epoch 137/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - loss: 0.7871 - mae: 0.6888 - mse: 0.7871 - val_loss: 0.5928 - val_mae: 0.5772 - val_mse: 0.5928\n",
      "Epoch 138/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - loss: 0.8204 - mae: 0.7182 - mse: 0.8204 - val_loss: 0.5928 - val_mae: 0.5774 - val_mse: 0.5928\n",
      "Epoch 139/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - loss: 0.8584 - mae: 0.7171 - mse: 0.8584 - val_loss: 0.5926 - val_mae: 0.5772 - val_mse: 0.5926\n",
      "Epoch 140/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 120ms/step - loss: 0.8189 - mae: 0.6932 - mse: 0.8189 - val_loss: 0.5923 - val_mae: 0.5771 - val_mse: 0.5923\n",
      "Epoch 141/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 94ms/step - loss: 0.8259 - mae: 0.7187 - mse: 0.8259 - val_loss: 0.5919 - val_mae: 0.5769 - val_mse: 0.5919\n",
      "Epoch 142/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 97ms/step - loss: 0.7999 - mae: 0.6985 - mse: 0.7999 - val_loss: 0.5915 - val_mae: 0.5766 - val_mse: 0.5915\n",
      "Epoch 143/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step - loss: 0.8403 - mae: 0.7076 - mse: 0.8403 - val_loss: 0.5911 - val_mae: 0.5763 - val_mse: 0.5911\n",
      "Epoch 144/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 141ms/step - loss: 0.8130 - mae: 0.7015 - mse: 0.8130 - val_loss: 0.5904 - val_mae: 0.5757 - val_mse: 0.5904\n",
      "Epoch 145/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 108ms/step - loss: 0.8020 - mae: 0.7007 - mse: 0.8020 - val_loss: 0.5890 - val_mae: 0.5745 - val_mse: 0.5890\n",
      "Epoch 146/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 110ms/step - loss: 0.8154 - mae: 0.7032 - mse: 0.8154 - val_loss: 0.5875 - val_mae: 0.5731 - val_mse: 0.5875\n",
      "Epoch 147/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - loss: 0.8267 - mae: 0.7029 - mse: 0.8267 - val_loss: 0.5859 - val_mae: 0.5716 - val_mse: 0.5859\n",
      "Epoch 148/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 119ms/step - loss: 0.8418 - mae: 0.7141 - mse: 0.8418 - val_loss: 0.5840 - val_mae: 0.5699 - val_mse: 0.5840\n",
      "Epoch 149/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 95ms/step - loss: 0.8452 - mae: 0.7143 - mse: 0.8452 - val_loss: 0.5822 - val_mae: 0.5682 - val_mse: 0.5822\n",
      "Epoch 150/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step - loss: 0.8769 - mae: 0.7375 - mse: 0.8769 - val_loss: 0.5803 - val_mae: 0.5665 - val_mse: 0.5803\n",
      "Epoch 151/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 96ms/step - loss: 0.8081 - mae: 0.6993 - mse: 0.8081 - val_loss: 0.5786 - val_mae: 0.5650 - val_mse: 0.5786\n",
      "Epoch 152/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - loss: 0.8466 - mae: 0.7157 - mse: 0.8466 - val_loss: 0.5769 - val_mae: 0.5635 - val_mse: 0.5769\n",
      "Epoch 153/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 0.7947 - mae: 0.6918 - mse: 0.7947 - val_loss: 0.5755 - val_mae: 0.5622 - val_mse: 0.5755\n",
      "Epoch 154/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - loss: 0.8097 - mae: 0.7094 - mse: 0.8097 - val_loss: 0.5741 - val_mae: 0.5610 - val_mse: 0.5741\n",
      "Epoch 155/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - loss: 0.8395 - mae: 0.7163 - mse: 0.8395 - val_loss: 0.5728 - val_mae: 0.5598 - val_mse: 0.5728\n",
      "Epoch 156/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - loss: 0.7909 - mae: 0.6991 - mse: 0.7909 - val_loss: 0.5719 - val_mae: 0.5590 - val_mse: 0.5719\n",
      "Epoch 157/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - loss: 0.8518 - mae: 0.7131 - mse: 0.8518 - val_loss: 0.5709 - val_mae: 0.5581 - val_mse: 0.5709\n",
      "Epoch 158/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - loss: 0.7877 - mae: 0.7009 - mse: 0.7877 - val_loss: 0.5701 - val_mae: 0.5574 - val_mse: 0.5701\n",
      "Epoch 159/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - loss: 0.8493 - mae: 0.7179 - mse: 0.8493 - val_loss: 0.5693 - val_mae: 0.5567 - val_mse: 0.5693\n",
      "Epoch 160/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - loss: 0.7787 - mae: 0.6868 - mse: 0.7787 - val_loss: 0.5688 - val_mae: 0.5563 - val_mse: 0.5688\n",
      "Epoch 161/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 102ms/step - loss: 0.7729 - mae: 0.6897 - mse: 0.7729 - val_loss: 0.5685 - val_mae: 0.5562 - val_mse: 0.5685\n",
      "Epoch 162/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - loss: 0.8079 - mae: 0.6939 - mse: 0.8079 - val_loss: 0.5685 - val_mae: 0.5563 - val_mse: 0.5685\n",
      "Epoch 163/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - loss: 0.7928 - mae: 0.6996 - mse: 0.7928 - val_loss: 0.5685 - val_mae: 0.5565 - val_mse: 0.5685\n",
      "Epoch 164/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 0.8565 - mae: 0.7342 - mse: 0.8565 - val_loss: 0.5686 - val_mae: 0.5567 - val_mse: 0.5686\n",
      "Epoch 165/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - loss: 0.8189 - mae: 0.7182 - mse: 0.8189 - val_loss: 0.5690 - val_mae: 0.5573 - val_mse: 0.5690\n",
      "Epoch 166/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step - loss: 0.8246 - mae: 0.7011 - mse: 0.8246 - val_loss: 0.5693 - val_mae: 0.5578 - val_mse: 0.5693\n",
      "Epoch 167/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 0.7915 - mae: 0.6945 - mse: 0.7915 - val_loss: 0.5697 - val_mae: 0.5583 - val_mse: 0.5697\n",
      "Epoch 168/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - loss: 0.8555 - mae: 0.7172 - mse: 0.8555 - val_loss: 0.5701 - val_mae: 0.5589 - val_mse: 0.5701\n",
      "Epoch 169/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - loss: 0.8157 - mae: 0.7035 - mse: 0.8157 - val_loss: 0.5706 - val_mae: 0.5595 - val_mse: 0.5706\n",
      "Epoch 170/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - loss: 0.7730 - mae: 0.6797 - mse: 0.7730 - val_loss: 0.5713 - val_mae: 0.5604 - val_mse: 0.5713\n",
      "Epoch 171/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 0.7845 - mae: 0.6902 - mse: 0.7845 - val_loss: 0.5718 - val_mae: 0.5611 - val_mse: 0.5718\n",
      "Epoch 172/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - loss: 0.8119 - mae: 0.7026 - mse: 0.8119 - val_loss: 0.5726 - val_mae: 0.5621 - val_mse: 0.5726\n",
      "Epoch 173/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 115ms/step - loss: 0.8067 - mae: 0.6948 - mse: 0.8067 - val_loss: 0.5733 - val_mae: 0.5629 - val_mse: 0.5733\n",
      "Epoch 174/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 0.8298 - mae: 0.7074 - mse: 0.8298 - val_loss: 0.5739 - val_mae: 0.5636 - val_mse: 0.5739\n",
      "Epoch 175/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - loss: 0.8131 - mae: 0.6952 - mse: 0.8131 - val_loss: 0.5747 - val_mae: 0.5646 - val_mse: 0.5747\n",
      "Epoch 176/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 98ms/step - loss: 0.7963 - mae: 0.6926 - mse: 0.7963 - val_loss: 0.5752 - val_mae: 0.5653 - val_mse: 0.5752\n",
      "Epoch 177/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - loss: 0.8467 - mae: 0.7193 - mse: 0.8467 - val_loss: 0.5756 - val_mae: 0.5658 - val_mse: 0.5756\n",
      "Epoch 178/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - loss: 0.8073 - mae: 0.6990 - mse: 0.8073 - val_loss: 0.5756 - val_mae: 0.5660 - val_mse: 0.5756\n",
      "Epoch 179/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - loss: 0.8113 - mae: 0.7053 - mse: 0.8113 - val_loss: 0.5757 - val_mae: 0.5662 - val_mse: 0.5757\n",
      "Epoch 180/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - loss: 0.8063 - mae: 0.7011 - mse: 0.8063 - val_loss: 0.5753 - val_mae: 0.5660 - val_mse: 0.5753\n",
      "Epoch 181/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 0.8117 - mae: 0.7200 - mse: 0.8117 - val_loss: 0.5751 - val_mae: 0.5659 - val_mse: 0.5751\n",
      "Epoch 182/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - loss: 0.8162 - mae: 0.6937 - mse: 0.8162 - val_loss: 0.5747 - val_mae: 0.5657 - val_mse: 0.5747\n",
      "Epoch 183/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - loss: 0.8193 - mae: 0.7176 - mse: 0.8193 - val_loss: 0.5746 - val_mae: 0.5657 - val_mse: 0.5746\n",
      "Epoch 184/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - loss: 0.7693 - mae: 0.6874 - mse: 0.7693 - val_loss: 0.5744 - val_mae: 0.5656 - val_mse: 0.5744\n",
      "Epoch 185/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step - loss: 0.7688 - mae: 0.6832 - mse: 0.7688 - val_loss: 0.5741 - val_mae: 0.5654 - val_mse: 0.5741\n",
      "Epoch 186/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 0.7994 - mae: 0.6984 - mse: 0.7994 - val_loss: 0.5742 - val_mae: 0.5656 - val_mse: 0.5742\n",
      "Epoch 187/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - loss: 0.8140 - mae: 0.7127 - mse: 0.8140 - val_loss: 0.5740 - val_mae: 0.5655 - val_mse: 0.5740\n",
      "Epoch 188/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - loss: 0.7504 - mae: 0.6861 - mse: 0.7504 - val_loss: 0.5739 - val_mae: 0.5655 - val_mse: 0.5739\n",
      "Epoch 189/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - loss: 0.7902 - mae: 0.6967 - mse: 0.7902 - val_loss: 0.5736 - val_mae: 0.5654 - val_mse: 0.5736\n",
      "Epoch 190/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step - loss: 0.8177 - mae: 0.6960 - mse: 0.8177 - val_loss: 0.5738 - val_mae: 0.5656 - val_mse: 0.5738\n",
      "Epoch 191/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - loss: 0.7343 - mae: 0.6783 - mse: 0.7343 - val_loss: 0.5740 - val_mae: 0.5660 - val_mse: 0.5740\n",
      "Epoch 192/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 94ms/step - loss: 0.7763 - mae: 0.6903 - mse: 0.7763 - val_loss: 0.5745 - val_mae: 0.5667 - val_mse: 0.5745\n",
      "Epoch 193/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 93ms/step - loss: 0.7256 - mae: 0.6695 - mse: 0.7256 - val_loss: 0.5745 - val_mae: 0.5668 - val_mse: 0.5745\n",
      "Epoch 194/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 0.8178 - mae: 0.7057 - mse: 0.8178 - val_loss: 0.5744 - val_mae: 0.5667 - val_mse: 0.5744\n",
      "Epoch 195/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step - loss: 0.7989 - mae: 0.6901 - mse: 0.7989 - val_loss: 0.5746 - val_mae: 0.5671 - val_mse: 0.5746\n",
      "Epoch 196/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - loss: 0.7977 - mae: 0.6921 - mse: 0.7977 - val_loss: 0.5751 - val_mae: 0.5677 - val_mse: 0.5751\n",
      "Epoch 197/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - loss: 0.8134 - mae: 0.7088 - mse: 0.8134 - val_loss: 0.5757 - val_mae: 0.5685 - val_mse: 0.5757\n",
      "Epoch 198/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - loss: 0.8215 - mae: 0.6947 - mse: 0.8215 - val_loss: 0.5763 - val_mae: 0.5692 - val_mse: 0.5763\n",
      "Epoch 199/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step - loss: 0.8023 - mae: 0.6896 - mse: 0.8023 - val_loss: 0.5770 - val_mae: 0.5700 - val_mse: 0.5770\n",
      "Epoch 200/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - loss: 0.8235 - mae: 0.7085 - mse: 0.8235 - val_loss: 0.5774 - val_mae: 0.5705 - val_mse: 0.5774\n",
      "Epoch 201/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - loss: 0.8142 - mae: 0.7012 - mse: 0.8142 - val_loss: 0.5781 - val_mae: 0.5714 - val_mse: 0.5781\n",
      "Epoch 202/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - loss: 0.7700 - mae: 0.6900 - mse: 0.7700 - val_loss: 0.5787 - val_mae: 0.5721 - val_mse: 0.5787\n",
      "Epoch 203/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - loss: 0.7850 - mae: 0.6889 - mse: 0.7850 - val_loss: 0.5795 - val_mae: 0.5730 - val_mse: 0.5795\n",
      "Epoch 204/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - loss: 0.8429 - mae: 0.7192 - mse: 0.8429 - val_loss: 0.5807 - val_mae: 0.5744 - val_mse: 0.5807\n",
      "Epoch 205/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - loss: 0.8073 - mae: 0.6985 - mse: 0.8073 - val_loss: 0.5818 - val_mae: 0.5756 - val_mse: 0.5818\n",
      "Epoch 206/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 0.8063 - mae: 0.7025 - mse: 0.8063 - val_loss: 0.5829 - val_mae: 0.5768 - val_mse: 0.5829\n",
      "Epoch 207/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 116ms/step - loss: 0.8563 - mae: 0.7279 - mse: 0.8563 - val_loss: 0.5838 - val_mae: 0.5779 - val_mse: 0.5838\n",
      "Epoch 208/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step - loss: 0.7938 - mae: 0.6978 - mse: 0.7938 - val_loss: 0.5846 - val_mae: 0.5788 - val_mse: 0.5846\n",
      "Epoch 209/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step - loss: 0.7667 - mae: 0.6900 - mse: 0.7667 - val_loss: 0.5850 - val_mae: 0.5793 - val_mse: 0.5850\n",
      "Epoch 210/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step - loss: 0.7783 - mae: 0.6963 - mse: 0.7783 - val_loss: 0.5851 - val_mae: 0.5796 - val_mse: 0.5851\n",
      "Epoch 211/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - loss: 0.8591 - mae: 0.7162 - mse: 0.8591 - val_loss: 0.5848 - val_mae: 0.5794 - val_mse: 0.5848\n",
      "Epoch 212/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - loss: 0.7818 - mae: 0.6862 - mse: 0.7818 - val_loss: 0.5846 - val_mae: 0.5793 - val_mse: 0.5846\n",
      "Epoch 213/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 0.7896 - mae: 0.6889 - mse: 0.7896 - val_loss: 0.5839 - val_mae: 0.5787 - val_mse: 0.5839\n",
      "Epoch 214/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 0.8004 - mae: 0.6946 - mse: 0.8004 - val_loss: 0.5828 - val_mae: 0.5777 - val_mse: 0.5828\n",
      "Epoch 215/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - loss: 0.8389 - mae: 0.7219 - mse: 0.8389 - val_loss: 0.5813 - val_mae: 0.5763 - val_mse: 0.5813\n",
      "Epoch 216/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 0.8172 - mae: 0.7067 - mse: 0.8172 - val_loss: 0.5801 - val_mae: 0.5751 - val_mse: 0.5801\n",
      "Epoch 217/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - loss: 0.8366 - mae: 0.7073 - mse: 0.8366 - val_loss: 0.5793 - val_mae: 0.5744 - val_mse: 0.5793\n",
      "Epoch 218/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step - loss: 0.8198 - mae: 0.7093 - mse: 0.8198 - val_loss: 0.5784 - val_mae: 0.5737 - val_mse: 0.5784\n",
      "Epoch 219/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step - loss: 0.7490 - mae: 0.6665 - mse: 0.7490 - val_loss: 0.5778 - val_mae: 0.5731 - val_mse: 0.5778\n",
      "Epoch 220/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - loss: 0.7923 - mae: 0.6891 - mse: 0.7923 - val_loss: 0.5772 - val_mae: 0.5727 - val_mse: 0.5772\n",
      "Epoch 221/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - loss: 0.8037 - mae: 0.7086 - mse: 0.8037 - val_loss: 0.5769 - val_mae: 0.5724 - val_mse: 0.5769\n",
      "Epoch 222/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - loss: 0.8068 - mae: 0.7071 - mse: 0.8068 - val_loss: 0.5762 - val_mae: 0.5718 - val_mse: 0.5762\n",
      "Epoch 223/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step - loss: 0.7528 - mae: 0.6723 - mse: 0.7528 - val_loss: 0.5754 - val_mae: 0.5711 - val_mse: 0.5754\n",
      "Epoch 224/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 97ms/step - loss: 0.7772 - mae: 0.6829 - mse: 0.7772 - val_loss: 0.5745 - val_mae: 0.5703 - val_mse: 0.5745\n",
      "Epoch 225/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step - loss: 0.7720 - mae: 0.6923 - mse: 0.7720 - val_loss: 0.5737 - val_mae: 0.5696 - val_mse: 0.5737\n",
      "Epoch 226/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - loss: 0.8185 - mae: 0.7038 - mse: 0.8185 - val_loss: 0.5731 - val_mae: 0.5690 - val_mse: 0.5731\n",
      "Epoch 227/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 0.8283 - mae: 0.7092 - mse: 0.8283 - val_loss: 0.5725 - val_mae: 0.5686 - val_mse: 0.5725\n",
      "Epoch 228/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - loss: 0.7723 - mae: 0.6818 - mse: 0.7723 - val_loss: 0.5725 - val_mae: 0.5686 - val_mse: 0.5725\n",
      "Epoch 229/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 94ms/step - loss: 0.7661 - mae: 0.6797 - mse: 0.7661 - val_loss: 0.5725 - val_mae: 0.5687 - val_mse: 0.5725\n",
      "Epoch 230/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - loss: 0.8011 - mae: 0.7039 - mse: 0.8011 - val_loss: 0.5734 - val_mae: 0.5698 - val_mse: 0.5734\n",
      "Epoch 231/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - loss: 0.8205 - mae: 0.7047 - mse: 0.8205 - val_loss: 0.5752 - val_mae: 0.5717 - val_mse: 0.5752\n",
      "Epoch 232/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 0.8118 - mae: 0.7043 - mse: 0.8118 - val_loss: 0.5771 - val_mae: 0.5738 - val_mse: 0.5771\n",
      "Epoch 233/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - loss: 0.7864 - mae: 0.6907 - mse: 0.7864 - val_loss: 0.5789 - val_mae: 0.5758 - val_mse: 0.5789\n",
      "Epoch 234/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - loss: 0.7870 - mae: 0.6885 - mse: 0.7870 - val_loss: 0.5802 - val_mae: 0.5771 - val_mse: 0.5802\n",
      "Epoch 235/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step - loss: 0.8199 - mae: 0.7029 - mse: 0.8199 - val_loss: 0.5816 - val_mae: 0.5786 - val_mse: 0.5816\n",
      "Epoch 236/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - loss: 0.7430 - mae: 0.6816 - mse: 0.7430 - val_loss: 0.5826 - val_mae: 0.5798 - val_mse: 0.5826\n",
      "Epoch 237/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 0.7534 - mae: 0.6740 - mse: 0.7534 - val_loss: 0.5834 - val_mae: 0.5807 - val_mse: 0.5834\n",
      "Epoch 238/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - loss: 0.7906 - mae: 0.6947 - mse: 0.7906 - val_loss: 0.5844 - val_mae: 0.5818 - val_mse: 0.5844\n",
      "Epoch 239/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 95ms/step - loss: 0.7855 - mae: 0.6963 - mse: 0.7855 - val_loss: 0.5852 - val_mae: 0.5827 - val_mse: 0.5852\n",
      "Epoch 240/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step - loss: 0.7931 - mae: 0.6908 - mse: 0.7931 - val_loss: 0.5858 - val_mae: 0.5835 - val_mse: 0.5858\n",
      "Epoch 241/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - loss: 0.7492 - mae: 0.6765 - mse: 0.7492 - val_loss: 0.5862 - val_mae: 0.5839 - val_mse: 0.5862\n",
      "Epoch 242/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 0.7501 - mae: 0.6843 - mse: 0.7501 - val_loss: 0.5859 - val_mae: 0.5837 - val_mse: 0.5859\n",
      "Epoch 243/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - loss: 0.7395 - mae: 0.6756 - mse: 0.7395 - val_loss: 0.5854 - val_mae: 0.5833 - val_mse: 0.5854\n",
      "Epoch 244/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - loss: 0.7377 - mae: 0.6785 - mse: 0.7377 - val_loss: 0.5846 - val_mae: 0.5826 - val_mse: 0.5846\n",
      "Epoch 245/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step - loss: 0.7878 - mae: 0.6930 - mse: 0.7878 - val_loss: 0.5837 - val_mae: 0.5819 - val_mse: 0.5837\n",
      "Epoch 246/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step - loss: 0.7848 - mae: 0.6815 - mse: 0.7848 - val_loss: 0.5827 - val_mae: 0.5809 - val_mse: 0.5827\n",
      "Epoch 247/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step - loss: 0.7912 - mae: 0.6950 - mse: 0.7912 - val_loss: 0.5813 - val_mae: 0.5796 - val_mse: 0.5813\n",
      "Epoch 248/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step - loss: 0.7919 - mae: 0.6774 - mse: 0.7919 - val_loss: 0.5792 - val_mae: 0.5776 - val_mse: 0.5792\n",
      "Epoch 249/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 0.7901 - mae: 0.6867 - mse: 0.7901 - val_loss: 0.5777 - val_mae: 0.5762 - val_mse: 0.5777\n",
      "Epoch 250/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step - loss: 0.7919 - mae: 0.6899 - mse: 0.7919 - val_loss: 0.5767 - val_mae: 0.5753 - val_mse: 0.5767\n",
      "Epoch 251/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - loss: 0.7575 - mae: 0.6829 - mse: 0.7575 - val_loss: 0.5751 - val_mae: 0.5738 - val_mse: 0.5751\n",
      "Epoch 252/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 97ms/step - loss: 0.7119 - mae: 0.6484 - mse: 0.7119 - val_loss: 0.5738 - val_mae: 0.5726 - val_mse: 0.5738\n",
      "Epoch 253/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 100ms/step - loss: 0.7928 - mae: 0.7085 - mse: 0.7928 - val_loss: 0.5726 - val_mae: 0.5715 - val_mse: 0.5726\n",
      "Epoch 254/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 111ms/step - loss: 0.8126 - mae: 0.7127 - mse: 0.8126 - val_loss: 0.5718 - val_mae: 0.5707 - val_mse: 0.5718\n",
      "Epoch 255/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 94ms/step - loss: 0.7761 - mae: 0.6778 - mse: 0.7761 - val_loss: 0.5713 - val_mae: 0.5702 - val_mse: 0.5713\n",
      "Epoch 256/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 0.7842 - mae: 0.6874 - mse: 0.7842 - val_loss: 0.5712 - val_mae: 0.5702 - val_mse: 0.5712\n",
      "Epoch 257/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - loss: 0.7600 - mae: 0.6813 - mse: 0.7600 - val_loss: 0.5714 - val_mae: 0.5706 - val_mse: 0.5714\n",
      "Epoch 258/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 0.7769 - mae: 0.6935 - mse: 0.7769 - val_loss: 0.5716 - val_mae: 0.5710 - val_mse: 0.5716\n",
      "Epoch 259/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - loss: 0.7436 - mae: 0.6721 - mse: 0.7436 - val_loss: 0.5720 - val_mae: 0.5714 - val_mse: 0.5720\n",
      "Epoch 260/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 0.7763 - mae: 0.6769 - mse: 0.7763 - val_loss: 0.5727 - val_mae: 0.5723 - val_mse: 0.5727\n",
      "Epoch 261/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - loss: 0.7400 - mae: 0.6685 - mse: 0.7400 - val_loss: 0.5738 - val_mae: 0.5735 - val_mse: 0.5738\n",
      "Epoch 262/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - loss: 0.8254 - mae: 0.6930 - mse: 0.8254 - val_loss: 0.5745 - val_mae: 0.5744 - val_mse: 0.5745\n",
      "Epoch 263/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 0.7596 - mae: 0.6794 - mse: 0.7596 - val_loss: 0.5759 - val_mae: 0.5759 - val_mse: 0.5759\n",
      "Epoch 264/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 0.7810 - mae: 0.6936 - mse: 0.7810 - val_loss: 0.5770 - val_mae: 0.5771 - val_mse: 0.5770\n",
      "Epoch 265/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 97ms/step - loss: 0.7887 - mae: 0.6828 - mse: 0.7887 - val_loss: 0.5781 - val_mae: 0.5784 - val_mse: 0.5781\n",
      "Epoch 266/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - loss: 0.7900 - mae: 0.6995 - mse: 0.7900 - val_loss: 0.5791 - val_mae: 0.5794 - val_mse: 0.5791\n",
      "Epoch 267/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - loss: 0.7654 - mae: 0.6831 - mse: 0.7654 - val_loss: 0.5804 - val_mae: 0.5809 - val_mse: 0.5804\n",
      "Epoch 268/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - loss: 0.8176 - mae: 0.6996 - mse: 0.8176 - val_loss: 0.5816 - val_mae: 0.5821 - val_mse: 0.5816\n",
      "Epoch 269/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step - loss: 0.8008 - mae: 0.7035 - mse: 0.8008 - val_loss: 0.5825 - val_mae: 0.5831 - val_mse: 0.5825\n",
      "Epoch 270/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - loss: 0.7507 - mae: 0.6581 - mse: 0.7507 - val_loss: 0.5833 - val_mae: 0.5840 - val_mse: 0.5833\n",
      "Epoch 271/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 0.8085 - mae: 0.6945 - mse: 0.8085 - val_loss: 0.5837 - val_mae: 0.5845 - val_mse: 0.5837\n",
      "Epoch 272/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - loss: 0.7972 - mae: 0.6980 - mse: 0.7972 - val_loss: 0.5831 - val_mae: 0.5840 - val_mse: 0.5831\n",
      "Epoch 273/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - loss: 0.7908 - mae: 0.6942 - mse: 0.7908 - val_loss: 0.5827 - val_mae: 0.5836 - val_mse: 0.5827\n",
      "Epoch 274/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - loss: 0.7606 - mae: 0.6667 - mse: 0.7606 - val_loss: 0.5830 - val_mae: 0.5841 - val_mse: 0.5830\n",
      "Epoch 275/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - loss: 0.7912 - mae: 0.6909 - mse: 0.7912 - val_loss: 0.5827 - val_mae: 0.5839 - val_mse: 0.5827\n",
      "Epoch 276/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - loss: 0.7941 - mae: 0.6966 - mse: 0.7941 - val_loss: 0.5830 - val_mae: 0.5843 - val_mse: 0.5830\n",
      "Epoch 277/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - loss: 0.7465 - mae: 0.6660 - mse: 0.7465 - val_loss: 0.5830 - val_mae: 0.5843 - val_mse: 0.5830\n",
      "Epoch 278/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 99ms/step - loss: 0.7810 - mae: 0.6805 - mse: 0.7810 - val_loss: 0.5830 - val_mae: 0.5845 - val_mse: 0.5830\n",
      "Epoch 279/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - loss: 0.7951 - mae: 0.6938 - mse: 0.7951 - val_loss: 0.5828 - val_mae: 0.5844 - val_mse: 0.5828\n",
      "Epoch 280/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - loss: 0.7470 - mae: 0.6698 - mse: 0.7470 - val_loss: 0.5827 - val_mae: 0.5844 - val_mse: 0.5827\n",
      "Epoch 281/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step - loss: 0.7590 - mae: 0.6938 - mse: 0.7590 - val_loss: 0.5826 - val_mae: 0.5844 - val_mse: 0.5826\n",
      "Epoch 282/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 107ms/step - loss: 0.8101 - mae: 0.6899 - mse: 0.8101 - val_loss: 0.5824 - val_mae: 0.5843 - val_mse: 0.5824\n",
      "Epoch 283/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step - loss: 0.8066 - mae: 0.6975 - mse: 0.8066 - val_loss: 0.5828 - val_mae: 0.5847 - val_mse: 0.5828\n",
      "Epoch 284/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - loss: 0.7863 - mae: 0.6847 - mse: 0.7863 - val_loss: 0.5832 - val_mae: 0.5853 - val_mse: 0.5832\n",
      "Epoch 285/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 0.8221 - mae: 0.7085 - mse: 0.8221 - val_loss: 0.5833 - val_mae: 0.5855 - val_mse: 0.5833\n",
      "Epoch 286/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step - loss: 0.7691 - mae: 0.6851 - mse: 0.7691 - val_loss: 0.5830 - val_mae: 0.5853 - val_mse: 0.5830\n",
      "Epoch 287/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - loss: 0.7956 - mae: 0.6937 - mse: 0.7956 - val_loss: 0.5827 - val_mae: 0.5851 - val_mse: 0.5827\n",
      "Epoch 288/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 0.7990 - mae: 0.6899 - mse: 0.7990 - val_loss: 0.5827 - val_mae: 0.5852 - val_mse: 0.5827\n",
      "Epoch 289/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - loss: 0.7395 - mae: 0.6635 - mse: 0.7395 - val_loss: 0.5827 - val_mae: 0.5853 - val_mse: 0.5827\n",
      "Epoch 290/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 96ms/step - loss: 0.7810 - mae: 0.6809 - mse: 0.7810 - val_loss: 0.5829 - val_mae: 0.5856 - val_mse: 0.5829\n",
      "Epoch 291/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - loss: 0.7727 - mae: 0.6838 - mse: 0.7727 - val_loss: 0.5834 - val_mae: 0.5863 - val_mse: 0.5834\n",
      "Epoch 292/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - loss: 0.7422 - mae: 0.6746 - mse: 0.7422 - val_loss: 0.5841 - val_mae: 0.5870 - val_mse: 0.5841\n",
      "Epoch 293/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step - loss: 0.7806 - mae: 0.6903 - mse: 0.7806 - val_loss: 0.5847 - val_mae: 0.5878 - val_mse: 0.5847\n",
      "Epoch 294/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - loss: 0.7918 - mae: 0.6975 - mse: 0.7918 - val_loss: 0.5854 - val_mae: 0.5885 - val_mse: 0.5854\n",
      "Epoch 295/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 0.8113 - mae: 0.7009 - mse: 0.8113 - val_loss: 0.5858 - val_mae: 0.5890 - val_mse: 0.5858\n",
      "Epoch 296/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104ms/step - loss: 0.7481 - mae: 0.6655 - mse: 0.7481 - val_loss: 0.5857 - val_mae: 0.5890 - val_mse: 0.5857\n",
      "Epoch 297/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 112ms/step - loss: 0.8247 - mae: 0.7046 - mse: 0.8247 - val_loss: 0.5856 - val_mae: 0.5890 - val_mse: 0.5856\n",
      "Epoch 298/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - loss: 0.7747 - mae: 0.6758 - mse: 0.7747 - val_loss: 0.5853 - val_mae: 0.5888 - val_mse: 0.5853\n",
      "Epoch 299/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 0.7756 - mae: 0.6836 - mse: 0.7756 - val_loss: 0.5849 - val_mae: 0.5885 - val_mse: 0.5849\n",
      "Epoch 300/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - loss: 0.7735 - mae: 0.6759 - mse: 0.7735 - val_loss: 0.5841 - val_mae: 0.5878 - val_mse: 0.5841\n",
      "Epoch 301/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 100ms/step - loss: 0.7727 - mae: 0.6832 - mse: 0.7727 - val_loss: 0.5828 - val_mae: 0.5865 - val_mse: 0.5828\n",
      "Epoch 302/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - loss: 0.7892 - mae: 0.6984 - mse: 0.7892 - val_loss: 0.5814 - val_mae: 0.5852 - val_mse: 0.5814\n",
      "Epoch 303/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 0.7703 - mae: 0.6895 - mse: 0.7703 - val_loss: 0.5807 - val_mae: 0.5846 - val_mse: 0.5807\n",
      "Epoch 304/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - loss: 0.7423 - mae: 0.6704 - mse: 0.7423 - val_loss: 0.5800 - val_mae: 0.5840 - val_mse: 0.5800\n",
      "Epoch 305/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - loss: 0.7911 - mae: 0.6848 - mse: 0.7911 - val_loss: 0.5791 - val_mae: 0.5832 - val_mse: 0.5791\n",
      "Epoch 306/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 0.8048 - mae: 0.7013 - mse: 0.8048 - val_loss: 0.5778 - val_mae: 0.5820 - val_mse: 0.5778\n",
      "Epoch 307/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - loss: 0.7904 - mae: 0.6949 - mse: 0.7904 - val_loss: 0.5770 - val_mae: 0.5812 - val_mse: 0.5770\n",
      "Epoch 308/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - loss: 0.7819 - mae: 0.6849 - mse: 0.7819 - val_loss: 0.5758 - val_mae: 0.5801 - val_mse: 0.5758\n",
      "Epoch 309/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step - loss: 0.7230 - mae: 0.6634 - mse: 0.7230 - val_loss: 0.5749 - val_mae: 0.5793 - val_mse: 0.5749\n",
      "Epoch 310/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 0.8197 - mae: 0.7058 - mse: 0.8197 - val_loss: 0.5744 - val_mae: 0.5789 - val_mse: 0.5744\n",
      "Epoch 311/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 0.7809 - mae: 0.6710 - mse: 0.7809 - val_loss: 0.5748 - val_mae: 0.5793 - val_mse: 0.5748\n",
      "Epoch 312/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step - loss: 0.7395 - mae: 0.6612 - mse: 0.7395 - val_loss: 0.5757 - val_mae: 0.5803 - val_mse: 0.5757\n",
      "Epoch 313/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step - loss: 0.7562 - mae: 0.6893 - mse: 0.7562 - val_loss: 0.5766 - val_mae: 0.5814 - val_mse: 0.5766\n",
      "Epoch 314/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - loss: 0.7862 - mae: 0.6797 - mse: 0.7862 - val_loss: 0.5779 - val_mae: 0.5828 - val_mse: 0.5779\n",
      "Epoch 315/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - loss: 0.7782 - mae: 0.6851 - mse: 0.7782 - val_loss: 0.5798 - val_mae: 0.5848 - val_mse: 0.5798\n",
      "Epoch 316/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - loss: 0.7982 - mae: 0.6885 - mse: 0.7982 - val_loss: 0.5816 - val_mae: 0.5867 - val_mse: 0.5816\n",
      "Epoch 317/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 0.7937 - mae: 0.7009 - mse: 0.7937 - val_loss: 0.5833 - val_mae: 0.5885 - val_mse: 0.5833\n",
      "Epoch 318/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - loss: 0.7631 - mae: 0.6788 - mse: 0.7631 - val_loss: 0.5850 - val_mae: 0.5902 - val_mse: 0.5850\n",
      "Epoch 319/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step - loss: 0.7775 - mae: 0.6888 - mse: 0.7775 - val_loss: 0.5861 - val_mae: 0.5915 - val_mse: 0.5861\n",
      "Epoch 320/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 0.7236 - mae: 0.6573 - mse: 0.7236 - val_loss: 0.5870 - val_mae: 0.5924 - val_mse: 0.5870\n",
      "Epoch 321/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - loss: 0.7511 - mae: 0.6752 - mse: 0.7511 - val_loss: 0.5878 - val_mae: 0.5933 - val_mse: 0.5878\n",
      "Epoch 322/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - loss: 0.7650 - mae: 0.6840 - mse: 0.7650 - val_loss: 0.5883 - val_mae: 0.5939 - val_mse: 0.5883\n",
      "Epoch 323/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 93ms/step - loss: 0.7834 - mae: 0.6855 - mse: 0.7834 - val_loss: 0.5884 - val_mae: 0.5941 - val_mse: 0.5884\n",
      "Epoch 324/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - loss: 0.8117 - mae: 0.7008 - mse: 0.8117 - val_loss: 0.5882 - val_mae: 0.5940 - val_mse: 0.5882\n",
      "Epoch 325/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 0.8049 - mae: 0.6929 - mse: 0.8049 - val_loss: 0.5881 - val_mae: 0.5940 - val_mse: 0.5881\n",
      "Epoch 326/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - loss: 0.8117 - mae: 0.6955 - mse: 0.8117 - val_loss: 0.5877 - val_mae: 0.5937 - val_mse: 0.5877\n",
      "Epoch 327/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - loss: 0.7481 - mae: 0.6656 - mse: 0.7481 - val_loss: 0.5873 - val_mae: 0.5933 - val_mse: 0.5873\n",
      "Epoch 328/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 117ms/step - loss: 0.7608 - mae: 0.6795 - mse: 0.7608 - val_loss: 0.5870 - val_mae: 0.5931 - val_mse: 0.5870\n",
      "Epoch 329/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - loss: 0.7714 - mae: 0.6837 - mse: 0.7714 - val_loss: 0.5862 - val_mae: 0.5925 - val_mse: 0.5862\n",
      "Epoch 330/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step - loss: 0.7683 - mae: 0.6967 - mse: 0.7683 - val_loss: 0.5863 - val_mae: 0.5926 - val_mse: 0.5863\n",
      "Epoch 331/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - loss: 0.7510 - mae: 0.6804 - mse: 0.7510 - val_loss: 0.5858 - val_mae: 0.5922 - val_mse: 0.5858\n",
      "Epoch 332/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 0.7528 - mae: 0.6768 - mse: 0.7528 - val_loss: 0.5859 - val_mae: 0.5923 - val_mse: 0.5859\n",
      "Epoch 333/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 97ms/step - loss: 0.7571 - mae: 0.6751 - mse: 0.7571 - val_loss: 0.5859 - val_mae: 0.5925 - val_mse: 0.5859\n",
      "Epoch 334/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - loss: 0.7812 - mae: 0.6915 - mse: 0.7812 - val_loss: 0.5862 - val_mae: 0.5928 - val_mse: 0.5862\n",
      "Epoch 335/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step - loss: 0.7498 - mae: 0.6656 - mse: 0.7498 - val_loss: 0.5871 - val_mae: 0.5939 - val_mse: 0.5871\n",
      "Epoch 336/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step - loss: 0.7272 - mae: 0.6722 - mse: 0.7272 - val_loss: 0.5881 - val_mae: 0.5948 - val_mse: 0.5881\n",
      "Epoch 337/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step - loss: 0.7336 - mae: 0.6677 - mse: 0.7336 - val_loss: 0.5881 - val_mae: 0.5950 - val_mse: 0.5881\n",
      "Epoch 338/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step - loss: 0.7125 - mae: 0.6541 - mse: 0.7125 - val_loss: 0.5881 - val_mae: 0.5951 - val_mse: 0.5881\n",
      "Epoch 339/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step - loss: 0.8227 - mae: 0.7142 - mse: 0.8227 - val_loss: 0.5871 - val_mae: 0.5942 - val_mse: 0.5871\n",
      "Epoch 340/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step - loss: 0.7281 - mae: 0.6707 - mse: 0.7281 - val_loss: 0.5855 - val_mae: 0.5926 - val_mse: 0.5855\n",
      "Epoch 341/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 94ms/step - loss: 0.7649 - mae: 0.6808 - mse: 0.7649 - val_loss: 0.5839 - val_mae: 0.5913 - val_mse: 0.5839\n",
      "Epoch 342/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 97ms/step - loss: 0.7238 - mae: 0.6663 - mse: 0.7238 - val_loss: 0.5825 - val_mae: 0.5899 - val_mse: 0.5825\n",
      "Epoch 343/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 172ms/step - loss: 0.7731 - mae: 0.6865 - mse: 0.7731 - val_loss: 0.5813 - val_mae: 0.5889 - val_mse: 0.5813\n",
      "Epoch 344/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 100ms/step - loss: 0.7380 - mae: 0.6779 - mse: 0.7380 - val_loss: 0.5806 - val_mae: 0.5882 - val_mse: 0.5806\n",
      "Epoch 345/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step - loss: 0.7628 - mae: 0.6747 - mse: 0.7628 - val_loss: 0.5803 - val_mae: 0.5880 - val_mse: 0.5803\n",
      "Epoch 346/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 93ms/step - loss: 0.7375 - mae: 0.6732 - mse: 0.7375 - val_loss: 0.5805 - val_mae: 0.5883 - val_mse: 0.5805\n",
      "Epoch 347/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - loss: 0.8013 - mae: 0.6986 - mse: 0.8013 - val_loss: 0.5804 - val_mae: 0.5883 - val_mse: 0.5804\n",
      "Epoch 348/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step - loss: 0.7834 - mae: 0.6855 - mse: 0.7834 - val_loss: 0.5807 - val_mae: 0.5887 - val_mse: 0.5807\n",
      "Epoch 349/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 94ms/step - loss: 0.7950 - mae: 0.7031 - mse: 0.7950 - val_loss: 0.5815 - val_mae: 0.5896 - val_mse: 0.5815\n",
      "Epoch 350/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step - loss: 0.7488 - mae: 0.6671 - mse: 0.7488 - val_loss: 0.5825 - val_mae: 0.5906 - val_mse: 0.5825\n",
      "Epoch 351/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step - loss: 0.7384 - mae: 0.6599 - mse: 0.7384 - val_loss: 0.5836 - val_mae: 0.5918 - val_mse: 0.5836\n",
      "Epoch 352/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step - loss: 0.7531 - mae: 0.6709 - mse: 0.7531 - val_loss: 0.5846 - val_mae: 0.5928 - val_mse: 0.5846\n",
      "Epoch 353/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104ms/step - loss: 0.7249 - mae: 0.6457 - mse: 0.7249 - val_loss: 0.5858 - val_mae: 0.5940 - val_mse: 0.5858\n",
      "Epoch 354/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step - loss: 0.7767 - mae: 0.6864 - mse: 0.7767 - val_loss: 0.5866 - val_mae: 0.5949 - val_mse: 0.5866\n",
      "Epoch 355/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - loss: 0.7305 - mae: 0.6560 - mse: 0.7305 - val_loss: 0.5872 - val_mae: 0.5955 - val_mse: 0.5872\n",
      "Epoch 356/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step - loss: 0.7528 - mae: 0.6753 - mse: 0.7528 - val_loss: 0.5882 - val_mae: 0.5966 - val_mse: 0.5882\n",
      "Epoch 357/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step - loss: 0.7388 - mae: 0.6645 - mse: 0.7388 - val_loss: 0.5892 - val_mae: 0.5977 - val_mse: 0.5892\n",
      "Epoch 358/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 95ms/step - loss: 0.7131 - mae: 0.6691 - mse: 0.7131 - val_loss: 0.5897 - val_mae: 0.5982 - val_mse: 0.5897\n",
      "Epoch 359/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 95ms/step - loss: 0.7720 - mae: 0.6866 - mse: 0.7720 - val_loss: 0.5901 - val_mae: 0.5987 - val_mse: 0.5901\n",
      "Epoch 360/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step - loss: 0.7653 - mae: 0.6908 - mse: 0.7653 - val_loss: 0.5898 - val_mae: 0.5984 - val_mse: 0.5898\n",
      "Epoch 361/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 95ms/step - loss: 0.7142 - mae: 0.6534 - mse: 0.7142 - val_loss: 0.5901 - val_mae: 0.5988 - val_mse: 0.5901\n",
      "Epoch 362/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - loss: 0.7378 - mae: 0.6700 - mse: 0.7378 - val_loss: 0.5898 - val_mae: 0.5985 - val_mse: 0.5898\n",
      "Epoch 363/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step - loss: 0.7101 - mae: 0.6622 - mse: 0.7101 - val_loss: 0.5895 - val_mae: 0.5983 - val_mse: 0.5895\n",
      "Epoch 364/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step - loss: 0.7145 - mae: 0.6605 - mse: 0.7145 - val_loss: 0.5895 - val_mae: 0.5984 - val_mse: 0.5895\n",
      "Epoch 365/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step - loss: 0.7722 - mae: 0.6918 - mse: 0.7722 - val_loss: 0.5898 - val_mae: 0.5986 - val_mse: 0.5898\n",
      "Epoch 366/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step - loss: 0.7183 - mae: 0.6689 - mse: 0.7183 - val_loss: 0.5907 - val_mae: 0.5996 - val_mse: 0.5907\n",
      "Epoch 367/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 95ms/step - loss: 0.7412 - mae: 0.6628 - mse: 0.7412 - val_loss: 0.5917 - val_mae: 0.6006 - val_mse: 0.5917\n",
      "Epoch 368/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step - loss: 0.8171 - mae: 0.7100 - mse: 0.8171 - val_loss: 0.5934 - val_mae: 0.6022 - val_mse: 0.5934\n",
      "Epoch 369/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 93ms/step - loss: 0.7329 - mae: 0.6675 - mse: 0.7329 - val_loss: 0.5952 - val_mae: 0.6039 - val_mse: 0.5952\n",
      "Epoch 370/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step - loss: 0.7639 - mae: 0.6716 - mse: 0.7639 - val_loss: 0.5967 - val_mae: 0.6054 - val_mse: 0.5967\n",
      "Epoch 371/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 150ms/step - loss: 0.7191 - mae: 0.6555 - mse: 0.7191 - val_loss: 0.5977 - val_mae: 0.6064 - val_mse: 0.5977\n",
      "Epoch 372/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 95ms/step - loss: 0.7252 - mae: 0.6695 - mse: 0.7252 - val_loss: 0.5986 - val_mae: 0.6072 - val_mse: 0.5986\n",
      "Epoch 373/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 94ms/step - loss: 0.7425 - mae: 0.6739 - mse: 0.7425 - val_loss: 0.5989 - val_mae: 0.6075 - val_mse: 0.5989\n",
      "Epoch 374/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step - loss: 0.7474 - mae: 0.6711 - mse: 0.7474 - val_loss: 0.5991 - val_mae: 0.6078 - val_mse: 0.5991\n",
      "Epoch 375/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step - loss: 0.7518 - mae: 0.6729 - mse: 0.7518 - val_loss: 0.5991 - val_mae: 0.6078 - val_mse: 0.5991\n",
      "Epoch 376/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step - loss: 0.7562 - mae: 0.6812 - mse: 0.7562 - val_loss: 0.5986 - val_mae: 0.6074 - val_mse: 0.5986\n",
      "Epoch 377/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step - loss: 0.7442 - mae: 0.6700 - mse: 0.7442 - val_loss: 0.5974 - val_mae: 0.6064 - val_mse: 0.5974\n",
      "Epoch 378/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 96ms/step - loss: 0.7714 - mae: 0.6841 - mse: 0.7714 - val_loss: 0.5964 - val_mae: 0.6055 - val_mse: 0.5964\n",
      "Epoch 379/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step - loss: 0.7257 - mae: 0.6567 - mse: 0.7257 - val_loss: 0.5952 - val_mae: 0.6045 - val_mse: 0.5952\n",
      "Epoch 380/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 112ms/step - loss: 0.7548 - mae: 0.6645 - mse: 0.7548 - val_loss: 0.5937 - val_mae: 0.6031 - val_mse: 0.5937\n",
      "Epoch 381/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step - loss: 0.7506 - mae: 0.6867 - mse: 0.7506 - val_loss: 0.5924 - val_mae: 0.6020 - val_mse: 0.5924\n",
      "Epoch 382/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 96ms/step - loss: 0.7398 - mae: 0.6634 - mse: 0.7398 - val_loss: 0.5913 - val_mae: 0.6010 - val_mse: 0.5913\n",
      "Epoch 383/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - loss: 0.7877 - mae: 0.6892 - mse: 0.7877 - val_loss: 0.5905 - val_mae: 0.6002 - val_mse: 0.5905\n",
      "Epoch 384/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step - loss: 0.7093 - mae: 0.6581 - mse: 0.7093 - val_loss: 0.5897 - val_mae: 0.5996 - val_mse: 0.5897\n",
      "Epoch 385/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 145ms/step - loss: 0.7072 - mae: 0.6640 - mse: 0.7072 - val_loss: 0.5899 - val_mae: 0.5999 - val_mse: 0.5899\n",
      "Epoch 386/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step - loss: 0.7205 - mae: 0.6579 - mse: 0.7205 - val_loss: 0.5904 - val_mae: 0.6004 - val_mse: 0.5904\n",
      "Epoch 387/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 94ms/step - loss: 0.7416 - mae: 0.6702 - mse: 0.7416 - val_loss: 0.5919 - val_mae: 0.6018 - val_mse: 0.5919\n",
      "Epoch 388/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step - loss: 0.6277 - mae: 0.6035 - mse: 0.6277 - val_loss: 0.5931 - val_mae: 0.6030 - val_mse: 0.5931\n",
      "Epoch 389/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 97ms/step - loss: 0.7607 - mae: 0.6733 - mse: 0.7607 - val_loss: 0.5947 - val_mae: 0.6046 - val_mse: 0.5947\n",
      "Epoch 390/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 93ms/step - loss: 0.7382 - mae: 0.6742 - mse: 0.7382 - val_loss: 0.5960 - val_mae: 0.6057 - val_mse: 0.5960\n",
      "Epoch 391/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 96ms/step - loss: 0.7651 - mae: 0.6767 - mse: 0.7651 - val_loss: 0.5963 - val_mae: 0.6061 - val_mse: 0.5963\n",
      "Epoch 392/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step - loss: 0.7135 - mae: 0.6480 - mse: 0.7135 - val_loss: 0.5967 - val_mae: 0.6065 - val_mse: 0.5967\n",
      "Epoch 393/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step - loss: 0.7491 - mae: 0.6787 - mse: 0.7491 - val_loss: 0.5971 - val_mae: 0.6069 - val_mse: 0.5971\n",
      "Epoch 394/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 94ms/step - loss: 0.7659 - mae: 0.6777 - mse: 0.7659 - val_loss: 0.5975 - val_mae: 0.6073 - val_mse: 0.5975\n",
      "Epoch 395/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - loss: 0.7442 - mae: 0.6768 - mse: 0.7442 - val_loss: 0.5979 - val_mae: 0.6076 - val_mse: 0.5979\n",
      "Epoch 396/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step - loss: 0.7647 - mae: 0.6826 - mse: 0.7647 - val_loss: 0.5983 - val_mae: 0.6080 - val_mse: 0.5983\n",
      "Epoch 397/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - loss: 0.7334 - mae: 0.6745 - mse: 0.7334 - val_loss: 0.5981 - val_mae: 0.6079 - val_mse: 0.5981\n",
      "Epoch 398/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 163ms/step - loss: 0.7368 - mae: 0.6634 - mse: 0.7368 - val_loss: 0.5985 - val_mae: 0.6083 - val_mse: 0.5985\n",
      "Epoch 399/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 102ms/step - loss: 0.7261 - mae: 0.6723 - mse: 0.7261 - val_loss: 0.5984 - val_mae: 0.6083 - val_mse: 0.5984\n",
      "Epoch 400/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 96ms/step - loss: 0.7285 - mae: 0.6590 - mse: 0.7285 - val_loss: 0.5976 - val_mae: 0.6076 - val_mse: 0.5976\n",
      "Epoch 401/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 97ms/step - loss: 0.7392 - mae: 0.6662 - mse: 0.7392 - val_loss: 0.5961 - val_mae: 0.6062 - val_mse: 0.5961\n",
      "Epoch 402/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 96ms/step - loss: 0.7277 - mae: 0.6634 - mse: 0.7277 - val_loss: 0.5948 - val_mae: 0.6050 - val_mse: 0.5948\n",
      "Epoch 403/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step - loss: 0.7302 - mae: 0.6581 - mse: 0.7302 - val_loss: 0.5933 - val_mae: 0.6038 - val_mse: 0.5933\n",
      "Epoch 404/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step - loss: 0.7243 - mae: 0.6588 - mse: 0.7243 - val_loss: 0.5920 - val_mae: 0.6027 - val_mse: 0.5920\n",
      "Epoch 405/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 94ms/step - loss: 0.7285 - mae: 0.6570 - mse: 0.7285 - val_loss: 0.5913 - val_mae: 0.6021 - val_mse: 0.5913\n",
      "Epoch 406/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 95ms/step - loss: 0.7594 - mae: 0.6874 - mse: 0.7594 - val_loss: 0.5912 - val_mae: 0.6020 - val_mse: 0.5912\n",
      "Epoch 407/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 94ms/step - loss: 0.7275 - mae: 0.6659 - mse: 0.7275 - val_loss: 0.5915 - val_mae: 0.6023 - val_mse: 0.5915\n",
      "Epoch 408/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step - loss: 0.7077 - mae: 0.6475 - mse: 0.7077 - val_loss: 0.5918 - val_mae: 0.6026 - val_mse: 0.5918\n",
      "Epoch 409/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 97ms/step - loss: 0.7444 - mae: 0.6660 - mse: 0.7444 - val_loss: 0.5918 - val_mae: 0.6027 - val_mse: 0.5918\n",
      "Epoch 410/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 151ms/step - loss: 0.7293 - mae: 0.6603 - mse: 0.7293 - val_loss: 0.5922 - val_mae: 0.6031 - val_mse: 0.5922\n",
      "Epoch 411/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 96ms/step - loss: 0.7136 - mae: 0.6587 - mse: 0.7136 - val_loss: 0.5928 - val_mae: 0.6037 - val_mse: 0.5928\n",
      "Epoch 412/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 93ms/step - loss: 0.7068 - mae: 0.6565 - mse: 0.7068 - val_loss: 0.5937 - val_mae: 0.6046 - val_mse: 0.5937\n",
      "Epoch 413/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step - loss: 0.7495 - mae: 0.6899 - mse: 0.7495 - val_loss: 0.5949 - val_mae: 0.6056 - val_mse: 0.5949\n",
      "Epoch 414/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 98ms/step - loss: 0.6758 - mae: 0.6411 - mse: 0.6758 - val_loss: 0.5960 - val_mae: 0.6067 - val_mse: 0.5960\n",
      "Epoch 415/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step - loss: 0.7206 - mae: 0.6594 - mse: 0.7206 - val_loss: 0.5984 - val_mae: 0.6087 - val_mse: 0.5984\n",
      "Epoch 416/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step - loss: 0.7073 - mae: 0.6489 - mse: 0.7073 - val_loss: 0.6010 - val_mae: 0.6111 - val_mse: 0.6010\n",
      "Epoch 417/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 95ms/step - loss: 0.7208 - mae: 0.6554 - mse: 0.7208 - val_loss: 0.6032 - val_mae: 0.6131 - val_mse: 0.6032\n",
      "Epoch 418/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 97ms/step - loss: 0.7456 - mae: 0.6743 - mse: 0.7456 - val_loss: 0.6050 - val_mae: 0.6147 - val_mse: 0.6050\n",
      "Epoch 419/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step - loss: 0.7067 - mae: 0.6647 - mse: 0.7067 - val_loss: 0.6070 - val_mae: 0.6165 - val_mse: 0.6070\n",
      "Epoch 420/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - loss: 0.7378 - mae: 0.6658 - mse: 0.7378 - val_loss: 0.6096 - val_mae: 0.6187 - val_mse: 0.6096\n",
      "Epoch 421/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 93ms/step - loss: 0.7592 - mae: 0.6783 - mse: 0.7592 - val_loss: 0.6124 - val_mae: 0.6212 - val_mse: 0.6124\n",
      "Epoch 422/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step - loss: 0.7373 - mae: 0.6681 - mse: 0.7373 - val_loss: 0.6139 - val_mae: 0.6224 - val_mse: 0.6139\n",
      "Epoch 423/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 141ms/step - loss: 0.6951 - mae: 0.6589 - mse: 0.6951 - val_loss: 0.6152 - val_mae: 0.6236 - val_mse: 0.6152\n",
      "Epoch 424/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 107ms/step - loss: 0.7402 - mae: 0.6644 - mse: 0.7402 - val_loss: 0.6166 - val_mae: 0.6247 - val_mse: 0.6166\n",
      "Epoch 425/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step - loss: 0.7192 - mae: 0.6545 - mse: 0.7192 - val_loss: 0.6172 - val_mae: 0.6252 - val_mse: 0.6172\n",
      "Epoch 426/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step - loss: 0.7324 - mae: 0.6696 - mse: 0.7324 - val_loss: 0.6179 - val_mae: 0.6258 - val_mse: 0.6179\n",
      "Epoch 427/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step - loss: 0.7182 - mae: 0.6556 - mse: 0.7182 - val_loss: 0.6196 - val_mae: 0.6272 - val_mse: 0.6196\n",
      "Epoch 428/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step - loss: 0.7480 - mae: 0.6746 - mse: 0.7480 - val_loss: 0.6205 - val_mae: 0.6280 - val_mse: 0.6205\n",
      "Epoch 429/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 99ms/step - loss: 0.7375 - mae: 0.6671 - mse: 0.7375 - val_loss: 0.6202 - val_mae: 0.6277 - val_mse: 0.6202\n",
      "Epoch 430/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 97ms/step - loss: 0.6942 - mae: 0.6475 - mse: 0.6942 - val_loss: 0.6204 - val_mae: 0.6279 - val_mse: 0.6204\n",
      "Epoch 431/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 110ms/step - loss: 0.6952 - mae: 0.6535 - mse: 0.6952 - val_loss: 0.6200 - val_mae: 0.6276 - val_mse: 0.6200\n",
      "Epoch 432/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step - loss: 0.7274 - mae: 0.6505 - mse: 0.7274 - val_loss: 0.6205 - val_mae: 0.6281 - val_mse: 0.6205\n",
      "Epoch 433/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step - loss: 0.7039 - mae: 0.6533 - mse: 0.7039 - val_loss: 0.6212 - val_mae: 0.6287 - val_mse: 0.6212\n",
      "Epoch 434/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step - loss: 0.7290 - mae: 0.6665 - mse: 0.7290 - val_loss: 0.6214 - val_mae: 0.6289 - val_mse: 0.6214\n",
      "Epoch 435/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step - loss: 0.7243 - mae: 0.6536 - mse: 0.7243 - val_loss: 0.6226 - val_mae: 0.6299 - val_mse: 0.6226\n",
      "Epoch 436/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 100ms/step - loss: 0.6737 - mae: 0.6434 - mse: 0.6737 - val_loss: 0.6230 - val_mae: 0.6302 - val_mse: 0.6230\n",
      "Epoch 437/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 93ms/step - loss: 0.7401 - mae: 0.6693 - mse: 0.7401 - val_loss: 0.6235 - val_mae: 0.6307 - val_mse: 0.6235\n",
      "Epoch 438/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step - loss: 0.7086 - mae: 0.6518 - mse: 0.7086 - val_loss: 0.6225 - val_mae: 0.6300 - val_mse: 0.6225\n",
      "Epoch 439/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - loss: 0.7335 - mae: 0.6656 - mse: 0.7335 - val_loss: 0.6209 - val_mae: 0.6288 - val_mse: 0.6209\n",
      "Epoch 440/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step - loss: 0.6820 - mae: 0.6443 - mse: 0.6820 - val_loss: 0.6194 - val_mae: 0.6277 - val_mse: 0.6194\n",
      "Epoch 441/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step - loss: 0.6780 - mae: 0.6360 - mse: 0.6780 - val_loss: 0.6172 - val_mae: 0.6260 - val_mse: 0.6172\n",
      "Epoch 442/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - loss: 0.6926 - mae: 0.6455 - mse: 0.6926 - val_loss: 0.6161 - val_mae: 0.6251 - val_mse: 0.6161\n",
      "Epoch 443/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step - loss: 0.7580 - mae: 0.6759 - mse: 0.7580 - val_loss: 0.6159 - val_mae: 0.6250 - val_mse: 0.6159\n",
      "Epoch 444/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 93ms/step - loss: 0.6776 - mae: 0.6404 - mse: 0.6776 - val_loss: 0.6154 - val_mae: 0.6247 - val_mse: 0.6154\n",
      "Epoch 445/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step - loss: 0.7008 - mae: 0.6485 - mse: 0.7008 - val_loss: 0.6154 - val_mae: 0.6248 - val_mse: 0.6154\n",
      "Epoch 446/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - loss: 0.7489 - mae: 0.6630 - mse: 0.7489 - val_loss: 0.6143 - val_mae: 0.6239 - val_mse: 0.6143\n",
      "Epoch 447/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 93ms/step - loss: 0.7159 - mae: 0.6559 - mse: 0.7159 - val_loss: 0.6133 - val_mae: 0.6232 - val_mse: 0.6133\n",
      "Epoch 448/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 145ms/step - loss: 0.7528 - mae: 0.6774 - mse: 0.7528 - val_loss: 0.6126 - val_mae: 0.6226 - val_mse: 0.6126\n",
      "Epoch 449/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 102ms/step - loss: 0.6964 - mae: 0.6467 - mse: 0.6964 - val_loss: 0.6125 - val_mae: 0.6227 - val_mse: 0.6125\n",
      "Epoch 450/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 95ms/step - loss: 0.7203 - mae: 0.6570 - mse: 0.7203 - val_loss: 0.6145 - val_mae: 0.6243 - val_mse: 0.6145\n",
      "Epoch 451/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step - loss: 0.7307 - mae: 0.6555 - mse: 0.7307 - val_loss: 0.6156 - val_mae: 0.6252 - val_mse: 0.6156\n",
      "Epoch 452/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step - loss: 0.7597 - mae: 0.6772 - mse: 0.7597 - val_loss: 0.6158 - val_mae: 0.6254 - val_mse: 0.6158\n",
      "Epoch 453/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step - loss: 0.7343 - mae: 0.6716 - mse: 0.7343 - val_loss: 0.6175 - val_mae: 0.6267 - val_mse: 0.6175\n",
      "Epoch 454/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step - loss: 0.7101 - mae: 0.6553 - mse: 0.7101 - val_loss: 0.6206 - val_mae: 0.6292 - val_mse: 0.6206\n",
      "Epoch 455/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step - loss: 0.7647 - mae: 0.6711 - mse: 0.7647 - val_loss: 0.6233 - val_mae: 0.6313 - val_mse: 0.6233\n",
      "Epoch 456/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step - loss: 0.7322 - mae: 0.6646 - mse: 0.7322 - val_loss: 0.6254 - val_mae: 0.6329 - val_mse: 0.6254\n",
      "Epoch 457/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step - loss: 0.7245 - mae: 0.6625 - mse: 0.7245 - val_loss: 0.6266 - val_mae: 0.6339 - val_mse: 0.6266\n",
      "Epoch 458/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - loss: 0.7223 - mae: 0.6496 - mse: 0.7223 - val_loss: 0.6281 - val_mae: 0.6350 - val_mse: 0.6281\n",
      "Epoch 459/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 98ms/step - loss: 0.7248 - mae: 0.6558 - mse: 0.7248 - val_loss: 0.6297 - val_mae: 0.6362 - val_mse: 0.6297\n",
      "Epoch 460/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step - loss: 0.6939 - mae: 0.6424 - mse: 0.6939 - val_loss: 0.6308 - val_mae: 0.6370 - val_mse: 0.6308\n",
      "Epoch 461/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 193ms/step - loss: 0.7142 - mae: 0.6557 - mse: 0.7142 - val_loss: 0.6310 - val_mae: 0.6371 - val_mse: 0.6310\n",
      "Epoch 462/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 99ms/step - loss: 0.7036 - mae: 0.6523 - mse: 0.7036 - val_loss: 0.6303 - val_mae: 0.6365 - val_mse: 0.6303\n",
      "Epoch 463/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 94ms/step - loss: 0.7181 - mae: 0.6480 - mse: 0.7181 - val_loss: 0.6303 - val_mae: 0.6364 - val_mse: 0.6303\n",
      "Epoch 464/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 100ms/step - loss: 0.7318 - mae: 0.6712 - mse: 0.7318 - val_loss: 0.6300 - val_mae: 0.6361 - val_mse: 0.6300\n",
      "Epoch 465/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step - loss: 0.7106 - mae: 0.6466 - mse: 0.7106 - val_loss: 0.6290 - val_mae: 0.6352 - val_mse: 0.6290\n",
      "Epoch 466/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step - loss: 0.7435 - mae: 0.6730 - mse: 0.7435 - val_loss: 0.6279 - val_mae: 0.6343 - val_mse: 0.6279\n",
      "Epoch 467/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step - loss: 0.6867 - mae: 0.6365 - mse: 0.6867 - val_loss: 0.6272 - val_mae: 0.6337 - val_mse: 0.6272\n",
      "Epoch 468/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 94ms/step - loss: 0.6930 - mae: 0.6354 - mse: 0.6930 - val_loss: 0.6259 - val_mae: 0.6326 - val_mse: 0.6259\n",
      "Epoch 469/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step - loss: 0.6726 - mae: 0.6323 - mse: 0.6726 - val_loss: 0.6252 - val_mae: 0.6319 - val_mse: 0.6252\n",
      "Epoch 470/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step - loss: 0.7149 - mae: 0.6586 - mse: 0.7149 - val_loss: 0.6244 - val_mae: 0.6312 - val_mse: 0.6244\n",
      "Epoch 471/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 113ms/step - loss: 0.7579 - mae: 0.6731 - mse: 0.7579 - val_loss: 0.6245 - val_mae: 0.6313 - val_mse: 0.6245\n",
      "Epoch 472/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 121ms/step - loss: 0.7167 - mae: 0.6510 - mse: 0.7167 - val_loss: 0.6255 - val_mae: 0.6322 - val_mse: 0.6255\n",
      "Epoch 473/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 96ms/step - loss: 0.6929 - mae: 0.6458 - mse: 0.6929 - val_loss: 0.6260 - val_mae: 0.6325 - val_mse: 0.6260\n",
      "Epoch 474/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step - loss: 0.7342 - mae: 0.6664 - mse: 0.7342 - val_loss: 0.6263 - val_mae: 0.6328 - val_mse: 0.6263\n",
      "Epoch 475/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 94ms/step - loss: 0.7324 - mae: 0.6602 - mse: 0.7324 - val_loss: 0.6265 - val_mae: 0.6330 - val_mse: 0.6265\n",
      "Epoch 476/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 95ms/step - loss: 0.6776 - mae: 0.6384 - mse: 0.6776 - val_loss: 0.6261 - val_mae: 0.6326 - val_mse: 0.6261\n",
      "Epoch 477/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 100ms/step - loss: 0.7437 - mae: 0.6683 - mse: 0.7437 - val_loss: 0.6245 - val_mae: 0.6314 - val_mse: 0.6245\n",
      "Epoch 478/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 95ms/step - loss: 0.7110 - mae: 0.6547 - mse: 0.7110 - val_loss: 0.6238 - val_mae: 0.6308 - val_mse: 0.6238\n",
      "Epoch 479/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step - loss: 0.7305 - mae: 0.6673 - mse: 0.7305 - val_loss: 0.6223 - val_mae: 0.6296 - val_mse: 0.6223\n",
      "Epoch 480/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step - loss: 0.7105 - mae: 0.6625 - mse: 0.7105 - val_loss: 0.6213 - val_mae: 0.6288 - val_mse: 0.6213\n",
      "Epoch 481/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 94ms/step - loss: 0.7238 - mae: 0.6651 - mse: 0.7238 - val_loss: 0.6194 - val_mae: 0.6274 - val_mse: 0.6194\n",
      "Epoch 482/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step - loss: 0.7845 - mae: 0.6938 - mse: 0.7845 - val_loss: 0.6177 - val_mae: 0.6261 - val_mse: 0.6177\n",
      "Epoch 483/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 102ms/step - loss: 0.7411 - mae: 0.6752 - mse: 0.7411 - val_loss: 0.6172 - val_mae: 0.6257 - val_mse: 0.6172\n",
      "Epoch 484/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 151ms/step - loss: 0.7184 - mae: 0.6543 - mse: 0.7184 - val_loss: 0.6165 - val_mae: 0.6251 - val_mse: 0.6165\n",
      "Epoch 485/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 95ms/step - loss: 0.7110 - mae: 0.6502 - mse: 0.7110 - val_loss: 0.6173 - val_mae: 0.6258 - val_mse: 0.6173\n",
      "Epoch 486/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 93ms/step - loss: 0.7296 - mae: 0.6646 - mse: 0.7296 - val_loss: 0.6194 - val_mae: 0.6277 - val_mse: 0.6194\n",
      "Epoch 487/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 93ms/step - loss: 0.6943 - mae: 0.6395 - mse: 0.6943 - val_loss: 0.6230 - val_mae: 0.6307 - val_mse: 0.6230\n",
      "Epoch 488/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step - loss: 0.7488 - mae: 0.6692 - mse: 0.7488 - val_loss: 0.6267 - val_mae: 0.6337 - val_mse: 0.6267\n",
      "Epoch 489/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 94ms/step - loss: 0.7447 - mae: 0.6806 - mse: 0.7447 - val_loss: 0.6309 - val_mae: 0.6370 - val_mse: 0.6309\n",
      "Epoch 490/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 95ms/step - loss: 0.6554 - mae: 0.6294 - mse: 0.6554 - val_loss: 0.6363 - val_mae: 0.6412 - val_mse: 0.6363\n",
      "Epoch 491/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 97ms/step - loss: 0.7051 - mae: 0.6522 - mse: 0.7051 - val_loss: 0.6404 - val_mae: 0.6443 - val_mse: 0.6404\n",
      "Epoch 492/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 93ms/step - loss: 0.6616 - mae: 0.6298 - mse: 0.6616 - val_loss: 0.6445 - val_mae: 0.6474 - val_mse: 0.6445\n",
      "Epoch 493/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 95ms/step - loss: 0.6959 - mae: 0.6550 - mse: 0.6959 - val_loss: 0.6467 - val_mae: 0.6492 - val_mse: 0.6467\n",
      "Epoch 494/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 110ms/step - loss: 0.7389 - mae: 0.6648 - mse: 0.7389 - val_loss: 0.6482 - val_mae: 0.6504 - val_mse: 0.6482\n",
      "Epoch 495/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 121ms/step - loss: 0.6715 - mae: 0.6489 - mse: 0.6715 - val_loss: 0.6469 - val_mae: 0.6496 - val_mse: 0.6469\n",
      "Epoch 496/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step - loss: 0.7000 - mae: 0.6621 - mse: 0.7000 - val_loss: 0.6448 - val_mae: 0.6480 - val_mse: 0.6448\n",
      "Epoch 497/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 98ms/step - loss: 0.7080 - mae: 0.6481 - mse: 0.7080 - val_loss: 0.6429 - val_mae: 0.6466 - val_mse: 0.6429\n",
      "Epoch 498/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 95ms/step - loss: 0.7127 - mae: 0.6504 - mse: 0.7127 - val_loss: 0.6397 - val_mae: 0.6442 - val_mse: 0.6397\n",
      "Epoch 499/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 93ms/step - loss: 0.7385 - mae: 0.6723 - mse: 0.7385 - val_loss: 0.6358 - val_mae: 0.6414 - val_mse: 0.6358\n",
      "Epoch 500/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step - loss: 0.7056 - mae: 0.6586 - mse: 0.7056 - val_loss: 0.6321 - val_mae: 0.6386 - val_mse: 0.6321\n",
      "Epoch 501/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 94ms/step - loss: 0.6838 - mae: 0.6442 - mse: 0.6838 - val_loss: 0.6297 - val_mae: 0.6368 - val_mse: 0.6297\n",
      "Epoch 502/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step - loss: 0.7181 - mae: 0.6592 - mse: 0.7181 - val_loss: 0.6295 - val_mae: 0.6366 - val_mse: 0.6295\n",
      "Epoch 503/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 94ms/step - loss: 0.6938 - mae: 0.6505 - mse: 0.6938 - val_loss: 0.6302 - val_mae: 0.6371 - val_mse: 0.6302\n",
      "Epoch 504/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 102ms/step - loss: 0.7498 - mae: 0.6835 - mse: 0.7498 - val_loss: 0.6321 - val_mae: 0.6385 - val_mse: 0.6321\n",
      "Epoch 505/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 95ms/step - loss: 0.7268 - mae: 0.6633 - mse: 0.7268 - val_loss: 0.6337 - val_mae: 0.6396 - val_mse: 0.6337\n",
      "Epoch 506/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step - loss: 0.7181 - mae: 0.6585 - mse: 0.7181 - val_loss: 0.6352 - val_mae: 0.6407 - val_mse: 0.6352\n",
      "Epoch 507/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - loss: 0.6953 - mae: 0.6540 - mse: 0.6953 - val_loss: 0.6358 - val_mae: 0.6410 - val_mse: 0.6358\n",
      "Epoch 508/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 100ms/step - loss: 0.7485 - mae: 0.6728 - mse: 0.7485 - val_loss: 0.6351 - val_mae: 0.6404 - val_mse: 0.6351\n",
      "Epoch 509/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 98ms/step - loss: 0.7092 - mae: 0.6544 - mse: 0.7092 - val_loss: 0.6343 - val_mae: 0.6398 - val_mse: 0.6343\n",
      "Epoch 510/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 100ms/step - loss: 0.6905 - mae: 0.6452 - mse: 0.6905 - val_loss: 0.6321 - val_mae: 0.6381 - val_mse: 0.6321\n",
      "Epoch 511/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104ms/step - loss: 0.7602 - mae: 0.6676 - mse: 0.7602 - val_loss: 0.6326 - val_mae: 0.6384 - val_mse: 0.6326\n",
      "Epoch 512/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step - loss: 0.7104 - mae: 0.6471 - mse: 0.7104 - val_loss: 0.6330 - val_mae: 0.6387 - val_mse: 0.6330\n",
      "Epoch 513/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 93ms/step - loss: 0.7093 - mae: 0.6410 - mse: 0.7093 - val_loss: 0.6344 - val_mae: 0.6397 - val_mse: 0.6344\n",
      "Epoch 514/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 98ms/step - loss: 0.6674 - mae: 0.6389 - mse: 0.6674 - val_loss: 0.6367 - val_mae: 0.6415 - val_mse: 0.6367\n",
      "Epoch 515/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step - loss: 0.7354 - mae: 0.6630 - mse: 0.7354 - val_loss: 0.6383 - val_mae: 0.6428 - val_mse: 0.6383\n",
      "Epoch 516/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 93ms/step - loss: 0.6792 - mae: 0.6323 - mse: 0.6792 - val_loss: 0.6400 - val_mae: 0.6441 - val_mse: 0.6400\n",
      "Epoch 517/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step - loss: 0.6919 - mae: 0.6429 - mse: 0.6919 - val_loss: 0.6424 - val_mae: 0.6460 - val_mse: 0.6424\n",
      "Epoch 518/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 108ms/step - loss: 0.7034 - mae: 0.6455 - mse: 0.7034 - val_loss: 0.6439 - val_mae: 0.6472 - val_mse: 0.6439\n",
      "Epoch 519/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 96ms/step - loss: 0.6947 - mae: 0.6526 - mse: 0.6947 - val_loss: 0.6455 - val_mae: 0.6485 - val_mse: 0.6455\n",
      "Epoch 520/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 147ms/step - loss: 0.7267 - mae: 0.6667 - mse: 0.7267 - val_loss: 0.6462 - val_mae: 0.6491 - val_mse: 0.6462\n",
      "Epoch 521/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 95ms/step - loss: 0.6731 - mae: 0.6390 - mse: 0.6731 - val_loss: 0.6465 - val_mae: 0.6494 - val_mse: 0.6465\n",
      "Epoch 522/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 94ms/step - loss: 0.6692 - mae: 0.6418 - mse: 0.6692 - val_loss: 0.6443 - val_mae: 0.6477 - val_mse: 0.6443\n",
      "Epoch 523/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 93ms/step - loss: 0.6784 - mae: 0.6343 - mse: 0.6784 - val_loss: 0.6441 - val_mae: 0.6476 - val_mse: 0.6441\n",
      "Epoch 524/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - loss: 0.6934 - mae: 0.6417 - mse: 0.6934 - val_loss: 0.6430 - val_mae: 0.6468 - val_mse: 0.6430\n",
      "Epoch 525/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 99ms/step - loss: 0.6927 - mae: 0.6405 - mse: 0.6927 - val_loss: 0.6407 - val_mae: 0.6450 - val_mse: 0.6407\n",
      "Epoch 526/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 94ms/step - loss: 0.6946 - mae: 0.6465 - mse: 0.6946 - val_loss: 0.6386 - val_mae: 0.6434 - val_mse: 0.6386\n",
      "Epoch 527/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step - loss: 0.7021 - mae: 0.6495 - mse: 0.7021 - val_loss: 0.6383 - val_mae: 0.6434 - val_mse: 0.6383\n",
      "Epoch 528/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 97ms/step - loss: 0.7116 - mae: 0.6575 - mse: 0.7116 - val_loss: 0.6399 - val_mae: 0.6447 - val_mse: 0.6399\n",
      "Epoch 529/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 93ms/step - loss: 0.6842 - mae: 0.6375 - mse: 0.6842 - val_loss: 0.6419 - val_mae: 0.6464 - val_mse: 0.6419\n",
      "Epoch 530/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 93ms/step - loss: 0.7027 - mae: 0.6449 - mse: 0.7027 - val_loss: 0.6453 - val_mae: 0.6492 - val_mse: 0.6453\n",
      "Epoch 531/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104ms/step - loss: 0.6968 - mae: 0.6458 - mse: 0.6968 - val_loss: 0.6480 - val_mae: 0.6513 - val_mse: 0.6480\n",
      "Epoch 532/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 93ms/step - loss: 0.6916 - mae: 0.6515 - mse: 0.6916 - val_loss: 0.6497 - val_mae: 0.6526 - val_mse: 0.6497\n",
      "Epoch 533/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - loss: 0.7023 - mae: 0.6464 - mse: 0.7023 - val_loss: 0.6503 - val_mae: 0.6531 - val_mse: 0.6503\n",
      "Epoch 534/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 97ms/step - loss: 0.7224 - mae: 0.6561 - mse: 0.7224 - val_loss: 0.6501 - val_mae: 0.6531 - val_mse: 0.6501\n",
      "Epoch 535/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step - loss: 0.7299 - mae: 0.6581 - mse: 0.7299 - val_loss: 0.6490 - val_mae: 0.6523 - val_mse: 0.6490\n",
      "Epoch 536/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 96ms/step - loss: 0.7298 - mae: 0.6609 - mse: 0.7298 - val_loss: 0.6454 - val_mae: 0.6496 - val_mse: 0.6454\n",
      "Epoch 537/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 100ms/step - loss: 0.6693 - mae: 0.6228 - mse: 0.6693 - val_loss: 0.6402 - val_mae: 0.6455 - val_mse: 0.6402\n",
      "Epoch 538/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step - loss: 0.7142 - mae: 0.6596 - mse: 0.7142 - val_loss: 0.6340 - val_mae: 0.6407 - val_mse: 0.6340\n",
      "Epoch 539/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 94ms/step - loss: 0.7211 - mae: 0.6626 - mse: 0.7211 - val_loss: 0.6327 - val_mae: 0.6396 - val_mse: 0.6327\n",
      "Epoch 540/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 93ms/step - loss: 0.6929 - mae: 0.6399 - mse: 0.6929 - val_loss: 0.6329 - val_mae: 0.6398 - val_mse: 0.6329\n",
      "Epoch 541/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 95ms/step - loss: 0.7034 - mae: 0.6478 - mse: 0.7034 - val_loss: 0.6331 - val_mae: 0.6401 - val_mse: 0.6331\n",
      "Epoch 542/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step - loss: 0.7014 - mae: 0.6381 - mse: 0.7014 - val_loss: 0.6356 - val_mae: 0.6421 - val_mse: 0.6356\n",
      "Epoch 543/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 142ms/step - loss: 0.6626 - mae: 0.6385 - mse: 0.6626 - val_loss: 0.6382 - val_mae: 0.6441 - val_mse: 0.6382\n",
      "Epoch 544/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 111ms/step - loss: 0.7073 - mae: 0.6497 - mse: 0.7073 - val_loss: 0.6411 - val_mae: 0.6465 - val_mse: 0.6411\n",
      "Epoch 545/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 93ms/step - loss: 0.7106 - mae: 0.6465 - mse: 0.7106 - val_loss: 0.6457 - val_mae: 0.6501 - val_mse: 0.6457\n",
      "Epoch 546/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 94ms/step - loss: 0.6877 - mae: 0.6437 - mse: 0.6877 - val_loss: 0.6502 - val_mae: 0.6536 - val_mse: 0.6502\n",
      "Epoch 547/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 93ms/step - loss: 0.6838 - mae: 0.6455 - mse: 0.6838 - val_loss: 0.6539 - val_mae: 0.6564 - val_mse: 0.6539\n",
      "Epoch 548/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step - loss: 0.7282 - mae: 0.6657 - mse: 0.7282 - val_loss: 0.6559 - val_mae: 0.6579 - val_mse: 0.6559\n",
      "Epoch 549/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 99ms/step - loss: 0.7149 - mae: 0.6608 - mse: 0.7149 - val_loss: 0.6567 - val_mae: 0.6584 - val_mse: 0.6567\n",
      "Epoch 550/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 101ms/step - loss: 0.7206 - mae: 0.6495 - mse: 0.7206 - val_loss: 0.6555 - val_mae: 0.6573 - val_mse: 0.6555\n",
      "Epoch 551/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step - loss: 0.7219 - mae: 0.6486 - mse: 0.7219 - val_loss: 0.6557 - val_mae: 0.6574 - val_mse: 0.6557\n",
      "Epoch 552/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step - loss: 0.6975 - mae: 0.6480 - mse: 0.6975 - val_loss: 0.6547 - val_mae: 0.6566 - val_mse: 0.6547\n",
      "Epoch 553/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step - loss: 0.7080 - mae: 0.6612 - mse: 0.7080 - val_loss: 0.6550 - val_mae: 0.6567 - val_mse: 0.6550\n",
      "Epoch 554/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step - loss: 0.6515 - mae: 0.6081 - mse: 0.6515 - val_loss: 0.6569 - val_mae: 0.6581 - val_mse: 0.6569\n",
      "Epoch 555/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step - loss: 0.6860 - mae: 0.6325 - mse: 0.6860 - val_loss: 0.6581 - val_mae: 0.6590 - val_mse: 0.6581\n",
      "Epoch 556/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 101ms/step - loss: 0.6612 - mae: 0.6353 - mse: 0.6612 - val_loss: 0.6579 - val_mae: 0.6588 - val_mse: 0.6579\n",
      "Epoch 557/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 109ms/step - loss: 0.6898 - mae: 0.6348 - mse: 0.6898 - val_loss: 0.6562 - val_mae: 0.6574 - val_mse: 0.6562\n",
      "Epoch 558/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step - loss: 0.6721 - mae: 0.6244 - mse: 0.6721 - val_loss: 0.6584 - val_mae: 0.6591 - val_mse: 0.6584\n",
      "Epoch 559/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step - loss: 0.6916 - mae: 0.6337 - mse: 0.6916 - val_loss: 0.6603 - val_mae: 0.6606 - val_mse: 0.6603\n",
      "Epoch 560/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step - loss: 0.6879 - mae: 0.6385 - mse: 0.6879 - val_loss: 0.6601 - val_mae: 0.6604 - val_mse: 0.6601\n",
      "Epoch 561/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 95ms/step - loss: 0.7162 - mae: 0.6535 - mse: 0.7162 - val_loss: 0.6599 - val_mae: 0.6603 - val_mse: 0.6599\n",
      "Epoch 562/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step - loss: 0.6893 - mae: 0.6495 - mse: 0.6893 - val_loss: 0.6595 - val_mae: 0.6599 - val_mse: 0.6595\n",
      "Epoch 563/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 103ms/step - loss: 0.6797 - mae: 0.6404 - mse: 0.6797 - val_loss: 0.6596 - val_mae: 0.6599 - val_mse: 0.6596\n",
      "Epoch 564/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 93ms/step - loss: 0.6805 - mae: 0.6297 - mse: 0.6805 - val_loss: 0.6594 - val_mae: 0.6598 - val_mse: 0.6594\n",
      "Epoch 565/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 112ms/step - loss: 0.7028 - mae: 0.6515 - mse: 0.7028 - val_loss: 0.6608 - val_mae: 0.6609 - val_mse: 0.6608\n",
      "Epoch 566/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - loss: 0.6938 - mae: 0.6507 - mse: 0.6938 - val_loss: 0.6623 - val_mae: 0.6623 - val_mse: 0.6623\n",
      "Epoch 567/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 93ms/step - loss: 0.7087 - mae: 0.6514 - mse: 0.7087 - val_loss: 0.6643 - val_mae: 0.6639 - val_mse: 0.6643\n",
      "Epoch 568/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step - loss: 0.7308 - mae: 0.6603 - mse: 0.7308 - val_loss: 0.6634 - val_mae: 0.6634 - val_mse: 0.6634\n",
      "Epoch 569/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104ms/step - loss: 0.6844 - mae: 0.6386 - mse: 0.6844 - val_loss: 0.6621 - val_mae: 0.6625 - val_mse: 0.6621\n",
      "Epoch 570/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 98ms/step - loss: 0.7015 - mae: 0.6465 - mse: 0.7015 - val_loss: 0.6605 - val_mae: 0.6613 - val_mse: 0.6605\n",
      "Epoch 571/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 94ms/step - loss: 0.6938 - mae: 0.6494 - mse: 0.6938 - val_loss: 0.6579 - val_mae: 0.6595 - val_mse: 0.6579\n",
      "Epoch 572/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 93ms/step - loss: 0.7015 - mae: 0.6445 - mse: 0.7015 - val_loss: 0.6567 - val_mae: 0.6588 - val_mse: 0.6567\n",
      "Epoch 573/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step - loss: 0.6347 - mae: 0.6081 - mse: 0.6347 - val_loss: 0.6547 - val_mae: 0.6575 - val_mse: 0.6547\n",
      "Epoch 574/3000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step - loss: 0.6444 - mae: 0.6226 - mse: 0.6444 - val_loss: 0.6543 - val_mae: 0.6574 - val_mse: 0.6543\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x7f21c2a62950>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_classes = 10\n",
    "MAX_EPOCHS = 3000\n",
    "lr = 0.0002\n",
    "bs = 1000\n",
    "\n",
    "\n",
    "x = w_mood_regression.train[0]\n",
    "LSTM_model_regression = Sequential([\n",
    "    Dense(32, activation=\"relu\", input_shape=(x.shape[1], x.shape[2])),\n",
    "    Dropout(0.2),\n",
    "    Dense(64, activation=\"relu\"),\n",
    "    LSTM(128),\n",
    "    Dropout(0.2),\n",
    "    Dense(64, activation=\"relu\"),\n",
    "    Dense(1)\n",
    "])\n",
    "#LSTM_model_regression.summary()\n",
    "\n",
    "compile_and_fit(LSTM_model_regression, w_mood_regression, patience=500, bs=bs, lr=lr, max_epochs=MAX_EPOCHS, regression=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original sample: 980, windowed data points: 765\n",
      "Original sample: 261, windowed data points: 87\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Dense, LSTM, Dropout\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "\n",
    "X_train, Y_train = w_mood.train\n",
    "X_test, Y_test = w_mood.test\n",
    "n_classes = 10\n",
    "repetitions = 2\n",
    "\n",
    "\n",
    "# function that should be maximized by the bayesian optimization\n",
    "def train_wrapper(bs: float, lr: float, lstm_units: float, encoding_layers: float, classification_layers: float, dropout: float):\n",
    "    batch_size = int(bs)\n",
    "    lstm_units = int(lstm_units)\n",
    "    encoding_layers = int(encoding_layers)\n",
    "    classification_layers = int(classification_layers)\n",
    "\n",
    "    # return negative value to turn a minimization to a maximization problem\n",
    "    results = [\n",
    "        -train_network(batch_size, lr, lstm_units, encoding_layers, classification_layers, dropout) for _ in range(repetitions)\n",
    "    ]\n",
    "    # average scores\n",
    "    return sum(results)/len(results)\n",
    "\n",
    "\n",
    "def train_network(bs: int, lr: float, lstm_units: int, encoding_layers: int, classification_layers: int, dropout: float) -> float:\n",
    "    encoding_layers_list = []\n",
    "    for i in range(encoding_layers):\n",
    "        encoding_layers_list += [Dense(2**(5+i), activation=\"relu\"), Dropout(dropout)]\n",
    "\n",
    "    classification_layers_list = []\n",
    "    for i in range(classification_layers):\n",
    "        classification_layers_list += [Dropout(dropout), Dense(2**(5+i), activation=\"relu\")]\n",
    "    classification_layers_list = classification_layers_list[::-1]\n",
    "    classification_layers_list = classification_layers_list[:-1]\n",
    "    LSTM_model = Sequential([\n",
    "        *encoding_layers_list,\n",
    "        LSTM(lstm_units),\n",
    "        *classification_layers_list,\n",
    "        Dense(n_classes, activation=\"softmax\")\n",
    "    ])\n",
    "    #print(LSTM_model.summary())\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=150,\n",
    "        mode='min'\n",
    "    )\n",
    "\n",
    "    LSTM_model.compile(\n",
    "        loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=lr),\n",
    "        metrics=[\"accuracy\", \"f1_score\"]\n",
    "    )\n",
    "\n",
    "    history = LSTM_model.fit(x=X_train, y=Y_train, epochs=1000, batch_size=bs,\n",
    "                        validation_data=(X_test, Y_test),\n",
    "                        callbacks=[early_stopping],\n",
    "                        verbose=0)\n",
    "    # return the lowest validation loss seen\n",
    "    return min(history.history[\"val_loss\"])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   |    bs     | classi... |  dropout  | encodi... |    lr     | lstm_u... |\n",
      "-------------------------------------------------------------------------------------------------\n",
      "| \u001b[0m1        \u001b[0m | \u001b[0m-1.1     \u001b[0m | \u001b[0m417.6    \u001b[0m | \u001b[0m3.161    \u001b[0m | \u001b[0m0.0001144\u001b[0m | \u001b[0m1.907    \u001b[0m | \u001b[0m0.001468 \u001b[0m | \u001b[0m76.32    \u001b[0m |\n",
      "| \u001b[95m2        \u001b[0m | \u001b[95m-1.087   \u001b[0m | \u001b[95m187.1    \u001b[0m | \u001b[95m2.037    \u001b[0m | \u001b[95m0.3968   \u001b[0m | \u001b[95m2.616    \u001b[0m | \u001b[95m0.004192 \u001b[0m | \u001b[95m360.9    \u001b[0m |\n",
      "| \u001b[0m3        \u001b[0m | \u001b[0m-1.155   \u001b[0m | \u001b[0m187.7    \u001b[0m | \u001b[0m3.397    \u001b[0m | \u001b[0m0.6737   \u001b[0m | \u001b[0m2.618    \u001b[0m | \u001b[0m0.001605 \u001b[0m | \u001b[0m357.8    \u001b[0m |\n",
      "| \u001b[0m4        \u001b[0m | \u001b[0m-1.089   \u001b[0m | \u001b[0m598.1    \u001b[0m | \u001b[0m1.955    \u001b[0m | \u001b[0m0.6631   \u001b[0m | \u001b[0m2.345    \u001b[0m | \u001b[0m0.001722 \u001b[0m | \u001b[0m177.4    \u001b[0m |\n",
      "| \u001b[95m5        \u001b[0m | \u001b[95m-1.071   \u001b[0m | \u001b[95m897.3    \u001b[0m | \u001b[95m2.589    \u001b[0m | \u001b[95m0.059    \u001b[0m | \u001b[95m3.512    \u001b[0m | \u001b[95m0.002269 \u001b[0m | \u001b[95m120.9    \u001b[0m |\n",
      "| \u001b[0m6        \u001b[0m | \u001b[0m-1.078   \u001b[0m | \u001b[0m410.1    \u001b[0m | \u001b[0m1.367    \u001b[0m | \u001b[0m0.73     \u001b[0m | \u001b[0m3.604    \u001b[0m | \u001b[0m0.006629 \u001b[0m | \u001b[0m221.6    \u001b[0m |\n",
      "| \u001b[0m7        \u001b[0m | \u001b[0m-1.197   \u001b[0m | \u001b[0m916.9    \u001b[0m | \u001b[0m1.284    \u001b[0m | \u001b[0m0.9866   \u001b[0m | \u001b[0m3.841    \u001b[0m | \u001b[0m0.0001333\u001b[0m | \u001b[0m250.0    \u001b[0m |\n",
      "| \u001b[0m8        \u001b[0m | \u001b[0m-1.118   \u001b[0m | \u001b[0m694.3    \u001b[0m | \u001b[0m2.721    \u001b[0m | \u001b[0m0.3291   \u001b[0m | \u001b[0m3.129    \u001b[0m | \u001b[0m0.007974 \u001b[0m | \u001b[0m46.58    \u001b[0m |\n",
      "| \u001b[0m9        \u001b[0m | \u001b[0m-1.074   \u001b[0m | \u001b[0m202.6    \u001b[0m | \u001b[0m1.881    \u001b[0m | \u001b[0m0.1592   \u001b[0m | \u001b[0m1.841    \u001b[0m | \u001b[0m0.008115 \u001b[0m | \u001b[0m371.0    \u001b[0m |\n",
      "| \u001b[0m10       \u001b[0m | \u001b[0m-1.18    \u001b[0m | \u001b[0m5.613    \u001b[0m | \u001b[0m3.463    \u001b[0m | \u001b[0m0.5125   \u001b[0m | \u001b[0m2.61     \u001b[0m | \u001b[0m0.007384 \u001b[0m | \u001b[0m485.7    \u001b[0m |\n",
      "| \u001b[0m11       \u001b[0m | \u001b[0m-1.192   \u001b[0m | \u001b[0m35.05    \u001b[0m | \u001b[0m3.156    \u001b[0m | \u001b[0m0.8636   \u001b[0m | \u001b[0m1.901    \u001b[0m | \u001b[0m0.008467 \u001b[0m | \u001b[0m234.5    \u001b[0m |\n",
      "| \u001b[0m12       \u001b[0m | \u001b[0m-1.089   \u001b[0m | \u001b[0m898.2    \u001b[0m | \u001b[0m1.259    \u001b[0m | \u001b[0m0.5595   \u001b[0m | \u001b[0m1.764    \u001b[0m | \u001b[0m0.007556 \u001b[0m | \u001b[0m146.1    \u001b[0m |\n",
      "| \u001b[0m13       \u001b[0m | \u001b[0m-1.116   \u001b[0m | \u001b[0m988.9    \u001b[0m | \u001b[0m3.238    \u001b[0m | \u001b[0m0.3226   \u001b[0m | \u001b[0m2.123    \u001b[0m | \u001b[0m0.003261 \u001b[0m | \u001b[0m310.8    \u001b[0m |\n",
      "| \u001b[0m14       \u001b[0m | \u001b[0m-1.129   \u001b[0m | \u001b[0m758.6    \u001b[0m | \u001b[0m1.611    \u001b[0m | \u001b[0m0.2334   \u001b[0m | \u001b[0m2.756    \u001b[0m | \u001b[0m0.007667 \u001b[0m | \u001b[0m495.6    \u001b[0m |\n",
      "| \u001b[0m15       \u001b[0m | \u001b[0m-1.165   \u001b[0m | \u001b[0m874.1    \u001b[0m | \u001b[0m3.977    \u001b[0m | \u001b[0m0.3885   \u001b[0m | \u001b[0m3.589    \u001b[0m | \u001b[0m0.007915 \u001b[0m | \u001b[0m380.0    \u001b[0m |\n",
      "| \u001b[0m16       \u001b[0m | \u001b[0m-1.096   \u001b[0m | \u001b[0m18.39    \u001b[0m | \u001b[0m1.994    \u001b[0m | \u001b[0m0.4658   \u001b[0m | \u001b[0m1.861    \u001b[0m | \u001b[0m0.006369 \u001b[0m | \u001b[0m222.7    \u001b[0m |\n",
      "| \u001b[0m17       \u001b[0m | \u001b[0m-1.098   \u001b[0m | \u001b[0m560.5    \u001b[0m | \u001b[0m2.769    \u001b[0m | \u001b[0m0.7248   \u001b[0m | \u001b[0m3.276    \u001b[0m | \u001b[0m0.004367 \u001b[0m | \u001b[0m349.0    \u001b[0m |\n",
      "| \u001b[0m18       \u001b[0m | \u001b[0m-1.168   \u001b[0m | \u001b[0m77.32    \u001b[0m | \u001b[0m3.012    \u001b[0m | \u001b[0m0.6909   \u001b[0m | \u001b[0m3.057    \u001b[0m | \u001b[0m0.002799 \u001b[0m | \u001b[0m328.2    \u001b[0m |\n",
      "| \u001b[95m19       \u001b[0m | \u001b[95m-1.051   \u001b[0m | \u001b[95m831.9    \u001b[0m | \u001b[95m1.022    \u001b[0m | \u001b[95m0.03918  \u001b[0m | \u001b[95m3.423    \u001b[0m | \u001b[95m0.0009039\u001b[0m | \u001b[95m395.1    \u001b[0m |\n",
      "| \u001b[0m20       \u001b[0m | \u001b[0m-1.14    \u001b[0m | \u001b[0m93.44    \u001b[0m | \u001b[0m3.981    \u001b[0m | \u001b[0m0.4627   \u001b[0m | \u001b[0m1.418    \u001b[0m | \u001b[0m0.002957 \u001b[0m | \u001b[0m87.93    \u001b[0m |\n",
      "| \u001b[0m21       \u001b[0m | \u001b[0m-1.165   \u001b[0m | \u001b[0m156.8    \u001b[0m | \u001b[0m1.199    \u001b[0m | \u001b[0m0.9138   \u001b[0m | \u001b[0m1.295    \u001b[0m | \u001b[0m0.003117 \u001b[0m | \u001b[0m339.5    \u001b[0m |\n",
      "| \u001b[0m22       \u001b[0m | \u001b[0m-1.197   \u001b[0m | \u001b[0m406.3    \u001b[0m | \u001b[0m3.199    \u001b[0m | \u001b[0m0.8566   \u001b[0m | \u001b[0m3.972    \u001b[0m | \u001b[0m0.008545 \u001b[0m | \u001b[0m151.9    \u001b[0m |\n",
      "| \u001b[0m23       \u001b[0m | \u001b[0m-1.114   \u001b[0m | \u001b[0m186.3    \u001b[0m | \u001b[0m2.285    \u001b[0m | \u001b[0m0.375    \u001b[0m | \u001b[0m1.693    \u001b[0m | \u001b[0m0.006393 \u001b[0m | \u001b[0m360.9    \u001b[0m |\n",
      "| \u001b[0m24       \u001b[0m | \u001b[0m-1.168   \u001b[0m | \u001b[0m204.5    \u001b[0m | \u001b[0m3.478    \u001b[0m | \u001b[0m0.7813   \u001b[0m | \u001b[0m1.216    \u001b[0m | \u001b[0m0.008852 \u001b[0m | \u001b[0m373.1    \u001b[0m |\n",
      "| \u001b[0m25       \u001b[0m | \u001b[0m-1.132   \u001b[0m | \u001b[0m831.1    \u001b[0m | \u001b[0m2.346    \u001b[0m | \u001b[0m0.4305   \u001b[0m | \u001b[0m2.085    \u001b[0m | \u001b[0m0.008008 \u001b[0m | \u001b[0m396.8    \u001b[0m |\n",
      "| \u001b[0m26       \u001b[0m | \u001b[0m-1.115   \u001b[0m | \u001b[0m200.6    \u001b[0m | \u001b[0m2.008    \u001b[0m | \u001b[0m0.5157   \u001b[0m | \u001b[0m2.448    \u001b[0m | \u001b[0m0.0009516\u001b[0m | \u001b[0m370.0    \u001b[0m |\n",
      "| \u001b[0m27       \u001b[0m | \u001b[0m-1.086   \u001b[0m | \u001b[0m598.6    \u001b[0m | \u001b[0m1.233    \u001b[0m | \u001b[0m0.2968   \u001b[0m | \u001b[0m1.259    \u001b[0m | \u001b[0m0.008394 \u001b[0m | \u001b[0m177.5    \u001b[0m |\n",
      "| \u001b[0m28       \u001b[0m | \u001b[0m-1.108   \u001b[0m | \u001b[0m834.8    \u001b[0m | \u001b[0m1.698    \u001b[0m | \u001b[0m0.2337   \u001b[0m | \u001b[0m3.765    \u001b[0m | \u001b[0m0.008353 \u001b[0m | \u001b[0m395.4    \u001b[0m |\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 22\u001b[0m\n\u001b[1;32m      6\u001b[0m pbounds \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbs\u001b[39m\u001b[38;5;124m'\u001b[39m: (\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1000\u001b[39m),\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m'\u001b[39m: (\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0.01\u001b[39m),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdropout\u001b[39m\u001b[38;5;124m\"\u001b[39m: (\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     13\u001b[0m }\n\u001b[1;32m     16\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m BayesianOptimization(\n\u001b[1;32m     17\u001b[0m     f\u001b[38;5;241m=\u001b[39mtrain_wrapper,\n\u001b[1;32m     18\u001b[0m     pbounds\u001b[38;5;241m=\u001b[39mpbounds,\n\u001b[1;32m     19\u001b[0m     random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m     20\u001b[0m )\n\u001b[0;32m---> 22\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43minit_points\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_iter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28mprint\u001b[39m(optimizer\u001b[38;5;241m.\u001b[39mmax)\n",
      "File \u001b[0;32m~/Desktop/Projects/DMT/.venv/lib/python3.11/site-packages/bayes_opt/bayesian_optimization.py:310\u001b[0m, in \u001b[0;36mBayesianOptimization.maximize\u001b[0;34m(self, init_points, n_iter, acquisition_function, acq, kappa, kappa_decay, kappa_decay_delay, xi, **gp_params)\u001b[0m\n\u001b[1;32m    308\u001b[0m     x_probe \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msuggest(util)\n\u001b[1;32m    309\u001b[0m     iteration \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 310\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprobe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_probe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlazy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bounds_transformer \u001b[38;5;129;01mand\u001b[39;00m iteration \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    313\u001b[0m     \u001b[38;5;66;03m# The bounds transformer should only modify the bounds after\u001b[39;00m\n\u001b[1;32m    314\u001b[0m     \u001b[38;5;66;03m# the init_points points (only for the true iterations)\u001b[39;00m\n\u001b[1;32m    315\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_bounds(\n\u001b[1;32m    316\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bounds_transformer\u001b[38;5;241m.\u001b[39mtransform(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_space))\n",
      "File \u001b[0;32m~/Desktop/Projects/DMT/.venv/lib/python3.11/site-packages/bayes_opt/bayesian_optimization.py:208\u001b[0m, in \u001b[0;36mBayesianOptimization.probe\u001b[0;34m(self, params, lazy)\u001b[0m\n\u001b[1;32m    206\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_queue\u001b[38;5;241m.\u001b[39madd(params)\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 208\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_space\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprobe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch(Events\u001b[38;5;241m.\u001b[39mOPTIMIZATION_STEP)\n",
      "File \u001b[0;32m~/Desktop/Projects/DMT/.venv/lib/python3.11/site-packages/bayes_opt/target_space.py:236\u001b[0m, in \u001b[0;36mTargetSpace.probe\u001b[0;34m(self, params)\u001b[0m\n\u001b[1;32m    234\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_as_array(params)\n\u001b[1;32m    235\u001b[0m params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_keys, x))\n\u001b[0;32m--> 236\u001b[0m target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_constraint \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    239\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mregister(x, target)\n",
      "Cell \u001b[0;32mIn[31], line 19\u001b[0m, in \u001b[0;36mtrain_wrapper\u001b[0;34m(bs, lr, lstm_units, encoding_layers, classification_layers, dropout)\u001b[0m\n\u001b[1;32m     16\u001b[0m classification_layers \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(classification_layers)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# return negative value to turn a minimization to a maximization problem\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mtrain_network\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlstm_units\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding_layers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclassification_layers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdropout\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mrepetitions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# average scores\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msum\u001b[39m(results)\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mlen\u001b[39m(results)\n",
      "Cell \u001b[0;32mIn[31], line 20\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     16\u001b[0m classification_layers \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(classification_layers)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# return negative value to turn a minimization to a maximization problem\u001b[39;00m\n\u001b[1;32m     19\u001b[0m results \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m---> 20\u001b[0m     \u001b[38;5;241m-\u001b[39m\u001b[43mtrain_network\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlstm_units\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding_layers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclassification_layers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdropout\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(repetitions)\n\u001b[1;32m     21\u001b[0m ]\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# average scores\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msum\u001b[39m(results)\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mlen\u001b[39m(results)\n",
      "Cell \u001b[0;32mIn[31], line 55\u001b[0m, in \u001b[0;36mtrain_network\u001b[0;34m(bs, lr, lstm_units, encoding_layers, classification_layers, dropout)\u001b[0m\n\u001b[1;32m     43\u001b[0m early_stopping \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mcallbacks\u001b[38;5;241m.\u001b[39mEarlyStopping(\n\u001b[1;32m     44\u001b[0m     monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     45\u001b[0m     patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m150\u001b[39m,\n\u001b[1;32m     46\u001b[0m     mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmin\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     47\u001b[0m )\n\u001b[1;32m     49\u001b[0m LSTM_model\u001b[38;5;241m.\u001b[39mcompile(\n\u001b[1;32m     50\u001b[0m     loss\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlosses\u001b[38;5;241m.\u001b[39mCategoricalCrossentropy(),\n\u001b[1;32m     51\u001b[0m     optimizer\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39moptimizers\u001b[38;5;241m.\u001b[39mAdam(learning_rate\u001b[38;5;241m=\u001b[39mlr),\n\u001b[1;32m     52\u001b[0m     metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mf1_score\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     53\u001b[0m )\n\u001b[0;32m---> 55\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mLSTM_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mY_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_test\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mearly_stopping\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;66;03m# return the lowest validation loss seen\u001b[39;00m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mmin\u001b[39m(history\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "File \u001b[0;32m~/Desktop/Projects/DMT/.venv/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/Desktop/Projects/DMT/.venv/lib/python3.11/site-packages/keras/src/backend/tensorflow/trainer.py:329\u001b[0m, in \u001b[0;36mTensorFlowTrainer.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[1;32m    327\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator\u001b[38;5;241m.\u001b[39menumerate_epoch():\n\u001b[1;32m    328\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m--> 329\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    330\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_end(\n\u001b[1;32m    331\u001b[0m         step, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pythonify_logs(logs)\n\u001b[1;32m    332\u001b[0m     )\n\u001b[1;32m    333\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop_training:\n",
      "File \u001b[0;32m~/Desktop/Projects/DMT/.venv/lib/python3.11/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/Desktop/Projects/DMT/.venv/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:833\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    830\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 833\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    835\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    836\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/Desktop/Projects/DMT/.venv/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:878\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    875\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    876\u001b[0m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[1;32m    877\u001b[0m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[0;32m--> 878\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mtracing_compilation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_variable_creation_config\u001b[49m\n\u001b[1;32m    880\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    881\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_created_variables:\n\u001b[1;32m    882\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating variables on a non-first call to a function\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    883\u001b[0m                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m decorated with tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Desktop/Projects/DMT/.venv/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[0;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[1;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Projects/DMT/.venv/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py:1322\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1318\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1320\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1321\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1322\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1323\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1324\u001b[0m     args,\n\u001b[1;32m   1325\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1326\u001b[0m     executing_eagerly)\n\u001b[1;32m   1327\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/Desktop/Projects/DMT/.venv/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[0;32m~/Desktop/Projects/DMT/.venv/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[1;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[1;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[1;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[1;32m    261\u001b[0m     )\n",
      "File \u001b[0;32m~/Desktop/Projects/DMT/.venv/lib/python3.11/site-packages/tensorflow/python/eager/context.py:1500\u001b[0m, in \u001b[0;36mContext.call_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1498\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[1;32m   1499\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1500\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1501\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1503\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1504\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1505\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1506\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1507\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1508\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m   1509\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1510\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1514\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[1;32m   1515\u001b[0m   )\n",
      "File \u001b[0;32m~/Desktop/Projects/DMT/.venv/lib/python3.11/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# pip install bayesian-optimization\n",
    "from bayes_opt import BayesianOptimization\n",
    "\n",
    "\n",
    "# Bounded region of parameter space\n",
    "pbounds = {\n",
    "    'bs': (1, 1000),\n",
    "    'lr': (0, 0.01),\n",
    "    \"lstm_units\": (32, 512),\n",
    "    \"encoding_layers\": (1, 4),\n",
    "    \"classification_layers\": (1, 4),\n",
    "    \"dropout\": (0, 1)\n",
    "}\n",
    "\n",
    "\n",
    "optimizer = BayesianOptimization(\n",
    "    f=train_wrapper,\n",
    "    pbounds=pbounds,\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "optimizer.maximize(\n",
    "    init_points=2,\n",
    "    n_iter=50,\n",
    ")\n",
    "\n",
    "print(optimizer.max)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
